---
title: "CEC'17 Results Comparison"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Statistical Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, echo = FALSE, error= TRUE)
```


```{r load.libraries, message=FALSE, warning=FALSE}
library(rNPBST)
library(dplyr)
library(scmamp)
library(xtable)
library(ggplot2)
```


# Official Results of the Competition with mean results of the different iterations 

```{r}
read.csv("data-raw/RankingCEC.csv") %>%
  arrange(desc(Score)) %>% select(-Rank) -> official.cec
```

```{r score}
summary.cec <- cec17.final %>% group_by(Benchmark,Dimension) %>% 
  mutate(Rank = rank(Result)) %>%
  ungroup() %>%
  group_by(Algorithm, Dimension) %>% 
  summarise(Sum = sum(Result),
            SumRank = sum(Rank)) %>%
  ungroup() %>% 
  gather(variable, value, starts_with("Sum")) %>% 
  unite(temp, Dimension, variable, sep = ".") %>% 
  spread(temp, value) %>%
  dplyr::mutate(SE = 0.1*`10.Sum` + 0.2*`30.Sum` + 0.3*`50.Sum` + 0.4*`100.Sum`,
                Score1 = 50 * (1 - (SE - min(SE)) / SE),
                SR = 0.1*`10.SumRank` + 0.2*`30.SumRank` + 0.3*`50.SumRank` + 0.4*`100.SumRank`,
                Score2 = 50 * (1 - (SR - min(SR)) / SR),
                Score = Score1 + Score2) %>%
  select(Algorithm, Score1, Score2, Score) %>%
  filter(Algorithm != "MOS12") %>%
  arrange(desc(Score)) 
```

```{r}
cbind(official.cec, summary.cec) %>%
  xtable(caption = "CEC17 Results Scores with mean results",
         label = "tab:CEC17scores") %>%
  print(include.rownames=F)
``` 

# Non-parametric tests

```{r apply.np.test}
ApplyNPTest <- function(data, test){
    split(data, data$Dimension) %>%
    lapply(function(x)
        select(x, -Dimension) %>% 
        tidyr::spread(Algorithm, Result) %>%
        select(-Benchmark) %>%
        test %>% get("p.value", .)) %>%
    unlist
}
```

## Post-hoc tests

Then, we proceed to perform the post-hoc tests in order to determinate
the location of the differences between these algorithms. We use the
modification of the classic non-parametric tests to obtain the
$p$-value associated with each hypothesis, although we should adjust
these $p$-values with a post-hoc procedure.


```{r post.hoc}
ApplyNPTestPostHoc <- function(data, test, adjust, control = NULL){
  data %>%
    tidyr::spread(Algorithm, Result) %>%
    select(-Benchmark, -Dimension) %>%
    test(control = control) %>%
    adjust
}
```

### $n$ versus $n$ scenario

In the scenario of the statistical analysis of the results obtained
during a competition, we would not focus in the comparison between the
results of a single algorithm but we would make all the posible pairs,
so we would not use the control algorithm.

As in the scenario with a control algorithm, we could be interested in
the  global comparison  between  all the  posible  pairs of  algorithm
considering the  results in the  different dimensions as if  they came
from the same distribution or pay attention to the differences between
the algorithms in the different contexts according to the dimension.

```{r all.vs.all}
cec17.final.10 <- filter(cec17.final, Dimension == 10)
ApplyNPTestPostHoc(cec17.final.10,
                   scmamp::friedmanPost,
                   scmamp::adjustHolland)  %>%
  AdjustFormatTable(caption = "Results $n$ vs $n$. Friedman test + Holland adjust.",
                    label = "tab:nvsn-friedman-holland", print.code = T)
```

```{r all.vs.all.group}
all.vs.all.group <- scmamp::postHocTest(select(tidyr::spread(cec17.final, Algorithm, Result),
                                             -Benchmark),
                                      group.by = "Dimension",
                                      test = "friedman", adjust = "holland")

all.vs.all.group$corrected.pval[, ,1] %>%
  AdjustFormatTable(caption = "Results $n$ vs $n$, dimension 10. Friedman test + Holland adjust.",
                    label = "tab:nvsn-dim10-friedman-holland")
```

```{r}
results.matrix.10 <- filter(cec17.final, Dimension == 10) %>% spread("Algorithm", "Result") %>% select(-Benchmark, -Dimension)
scmamp::plotCD(-results.matrix.10)

results.matrix.30 <- filter(cec17.final, Dimension == 30) %>% spread("Algorithm", "Result") %>% select(-Benchmark, -Dimension)
scmamp::plotCD(-results.matrix.30)

results.matrix.50 <- filter(cec17.final, Dimension == 50) %>% spread("Algorithm", "Result") %>% select(-Benchmark, -Dimension)
scmamp::plotCD(-results.matrix.50)

results.matrix.100 <- filter(cec17.final, Dimension == 100) %>% spread("Algorithm", "Result") %>% select(-Benchmark, -Dimension)
scmamp::plotCD(-results.matrix.100)
```

```{r}
results.matrix.agg <- 0.1*results.matrix.10 + 0.2*results.matrix.30 + 0.3*results.matrix.50 + 0.4*results.matrix.100
scmamp::plotCD(-results.matrix.agg)
```


```{r}
pvalues<- friedmanPost(results.matrix.10) 
ordering <- order(desc(summarizeData(results.matrix.10, fun = median)))
plotPvalues(pvalues,ordering)
scmamp::drawAlgorithmGraph(pvalues,summarizeData(results.matrix.10, fun = median))
```


# Bayesian tests

## Bayesian Sign and Signed-Rank test

```{r}
apply.paired.bayesian <- function(matrix.dataset, test,...){
  m <- ncol(matrix.dataset)
  comb <- utils::combn(m, 2)
  
  test.result <- apply(comb, 2, function(c){
                        single.result <- do.call(test, list(x = matrix.dataset[ ,c], ...))$probabilities
                        return(single.result)
                      }) %>%
    t %>% as.data.frame() %>%
    dplyr::mutate(Algorithm.1 = as.factor(colnames(matrix.dataset)[comb[1, ]]),
                  Algorithm.2 = as.factor(colnames(matrix.dataset)[comb[2, ]]))
  return(test.result)
}
```


```{r bayesian.pair.test.10}
results.matrix.10 <- filter(cec17.final, Dimension == 10, Algorithm != "MOS12") %>% 
  spread("Algorithm", "Result") %>% select(-Benchmark, -Dimension) %>%
  apply.paired.bayesian(rNPBST::bayesianSignedRank.test, rope.min = -10, rope.max = 10)

plot.10 <- results.matrix.10 %>%
  mutate(Probability = pmax(left, rope, right),
         Choice = factor(apply(.[1:3], 1, function(x) names(x)[which.max(x)]), levels = c("left", "right", "rope")),
         Algorithm.1 = as.character(Algorithm.1),
         Algorithm.2 = as.character(Algorithm.2)) %>%
  select(Algorithm.1, Algorithm.2, Probability, Choice) %>%
  rbind(., 
      transform(., Algorithm.1 = Algorithm.2, Algorithm.2 = Algorithm.1, 
                Choice = case_when(Choice == "left" ~ "right", 
                                   Choice == "right" ~ "left",
                                   Choice == "rope" ~ "rope"))) %>%
  mutate(Algorithm.1 = as.factor(Algorithm.1),
         Algorithm.2 = as.factor(Algorithm.2),
         Exceed = format(Probability, digits = 2)) %>% 
  ggplot2::ggplot(aes(y = reorder(Algorithm.1, desc(Algorithm.1)), 
                      x = Algorithm.2, alpha = Probability, color = Choice, fill = Choice)) +
  ggplot2::geom_tile() +
  scale_fill_discrete(drop=FALSE) +
  scale_alpha_continuous(guide=FALSE) +
  scale_color_discrete(drop=FALSE) +
  scale_y_discrete(limits = levels(.data$Algorithm.1)) +
  scale_x_discrete(limits = levels(rlang::.data$Algorithm.2)) +
  geom_text(aes(label=Exceed), colour = "black") +
  labs(x = "Algorithm 2", y = "Algorithm 1") +
  ggtitle("Dimension 10")
```

```{r bayesian.pair.test.30}
results.matrix.30 <- filter(cec17.final, Dimension == 30, Algorithm != "MOS12") %>% 
  spread("Algorithm", "Result") %>% select(-Benchmark, -Dimension) %>%
  apply.paired.bayesian(rNPBST::bayesianSignedRank.test, rope.min = -10, rope.max = 10)

plot.30 <- results.matrix.30 %>%
  mutate(Probability = pmax(left, rope, right),
         Choice = factor(apply(.[1:3], 1, function(x) names(x)[which.max(x)]), levels = c("left", "right", "rope")),
         Algorithm.1 = as.character(Algorithm.1),
         Algorithm.2 = as.character(Algorithm.2)) %>%
  select(Algorithm.1, Algorithm.2, Probability, Choice) %>%
  rbind(., 
      transform(., Algorithm.1 = Algorithm.2, Algorithm.2 = Algorithm.1, 
                Choice = case_when(Choice == "left" ~ "right", 
                                   Choice == "right" ~ "left",
                                   Choice == "rope" ~ "rope"))) %>%
  mutate(Algorithm.1 = as.factor(Algorithm.1),
         Algorithm.2 = as.factor(Algorithm.2),
         Exceed = format(Probability, digits = 2)) %>% 
  ggplot2::ggplot(aes(y = reorder(Algorithm.1, desc(Algorithm.1)), 
                      x = Algorithm.2, alpha = Probability, color = Choice, fill = Choice)) +
  ggplot2::geom_tile() +
  scale_fill_discrete(drop=FALSE) +
  scale_alpha_continuous(guide=FALSE) +
  scale_color_discrete(drop=FALSE) +
  scale_y_discrete(limits = levels(.data$Algorithm.1)) +
  scale_x_discrete(limits = levels(rlang::.data$Algorithm.2)) +
  geom_text(aes(label=Exceed), colour = "black") +
  labs(x = "Algorithm 2", y = "Algorithm 1") +
  ggtitle("Dimension 30")
```

```{r bayesian.pair.test.50}
results.matrix.50 <- filter(cec17.final, Dimension == 50, Algorithm != "MOS12") %>% 
  spread("Algorithm", "Result") %>% select(-Benchmark, -Dimension) %>%
  apply.paired.bayesian(rNPBST::bayesianSignedRank.test, rope.min = -10, rope.max = 10)

plot.50 <- results.matrix.50 %>%
  mutate(Probability = pmax(left, rope, right),
         Choice = factor(apply(.[1:3], 1, function(x) names(x)[which.max(x)]), levels = c("left", "right", "rope")),
         Algorithm.1 = as.character(Algorithm.1),
         Algorithm.2 = as.character(Algorithm.2)) %>%
  select(Algorithm.1, Algorithm.2, Probability, Choice) %>%
  rbind(., 
        transform(., Algorithm.1 = Algorithm.2, Algorithm.2 = Algorithm.1, 
                  Choice = case_when(Choice == "left" ~ "right", 
                                     Choice == "right" ~ "left",
                                     Choice == "rope" ~ "rope"))) %>%
  mutate(Algorithm.1 = as.factor(Algorithm.1),
         Algorithm.2 = as.factor(Algorithm.2),
         Exceed = format(Probability, digits = 2)) %>% 
  ggplot2::ggplot(aes(y = reorder(Algorithm.1, desc(Algorithm.1)), 
                      x = Algorithm.2, alpha = Probability, color = Choice, fill = Choice)) +
  ggplot2::geom_tile() +
  scale_fill_discrete(drop=FALSE) +
  scale_alpha_continuous(guide=FALSE) +
  scale_color_discrete(drop=FALSE) +
  scale_y_discrete(limits = levels(.data$Algorithm.1)) +
  scale_x_discrete(limits = levels(rlang::.data$Algorithm.2)) +
  geom_text(aes(label=Exceed), colour = "black") +
  labs(x = "Algorithm 2", y = "Algorithm 1") +
  ggtitle("Dimension 50")
```


```{r bayesian.pair.test.100}
results.matrix.100 <- filter(cec17.final, Dimension == 100, Algorithm != "MOS12") %>% 
  spread("Algorithm", "Result") %>% select(-Benchmark, -Dimension) %>%
  apply.paired.bayesian(rNPBST::bayesianSignedRank.test, rope.min = -10, rope.max = 10)

plot.100 <- results.matrix.100 %>%
  mutate(Probability = pmax(left, rope, right),
         Choice = factor(apply(.[1:3], 1, function(x) names(x)[which.max(x)]), levels = c("left", "right", "rope")),
         Algorithm.1 = as.character(Algorithm.1),
         Algorithm.2 = as.character(Algorithm.2)) %>%
  select(Algorithm.1, Algorithm.2, Probability, Choice) %>%
  rbind(., 
        transform(., Algorithm.1 = Algorithm.2, Algorithm.2 = Algorithm.1, 
                  Choice = case_when(Choice == "left" ~ "right", 
                                     Choice == "right" ~ "left",
                                     Choice == "rope" ~ "rope"))) %>%
  mutate(Algorithm.1 = as.factor(Algorithm.1),
         Algorithm.2 = as.factor(Algorithm.2),
         Exceed = format(Probability, digits = 2)) %>% 
  ggplot2::ggplot(aes(y = reorder(Algorithm.1, desc(Algorithm.1)), 
                      x = Algorithm.2, alpha = Probability, color = Choice, fill = Choice)) +
  ggplot2::geom_tile() +
  scale_fill_discrete(drop=FALSE) +
  scale_alpha_continuous(guide=FALSE) +
  scale_color_discrete(drop=FALSE) +
  scale_y_discrete(limits = levels(.data$Algorithm.1)) +
  scale_x_discrete(limits = levels(rlang::.data$Algorithm.2)) +
  geom_text(aes(label=Exceed), colour = "black") +
  labs(x = "Algorithm 2", y = "Algorithm 1") +
  ggtitle("Dimension 100")
```

```{r}
multiplot(plot.10, plot.50, plot.30, plot.100, cols = 2)
```


```{r}
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```
## PlackettLuce

```{r}
library(rNPBST)
cec17.extended.10 <- filter(cec17.extended, Dimension == "10") %>% select(-Dimension)
a <- merge(cec17.extended.10, cec17.extended.10, by = c("Benchmark","Iteration")) %>%
  filter(Algorithm.x != Algorithm.y) %>%
  filter_at(vars(ends_with(".x"),
                 ends_with(".y")), 
            any_vars(. == 0)) %>%
  group_by(Algorithm.x, Algorithm.y) %>%
  mutate(End.x = sum(!!any_vars(. == 0)))
  summarise(r_ij = n(),
            w_ij = sum(vars(ends_with(".x")), any_vars(. == 0)))) %>%
  ungroup()
```

# Appendix

```{r}
base.data <- cec17.final %>%
  filter(Dimension == 10) %>%
  select(-Dimension) %>%
  reshape(idvar = "Benchmark", timevar = "Algorithm", direction = "wide") %>%
  transform(Benchmark = as.integer(levels(Benchmark))[Benchmark]) %>% arrange(Benchmark)
colnames(base.data)[-1] <- levels(cec17.final$Algorithm)
knitr::kable(base.data)
```
