---
title: "Statistical Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Statistical Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, echo = FALSE, error= TRUE)
```


```{r load.libraries, message=FALSE, warning=FALSE}
library(rNPBST)
library(dplyr)
library(scmamp)
library(xtable)
library(ggplot2)
```

This vignette comprehends a statistical analysis of the case of study
of the CECâ€™17 Special Session and Competition on Single Objective Real
Parameter Numerical Optimization can be illustrated. The results have
been included in the package as a dataset.

- We will mainly use two data sets, one with all the results (all
  iterations of the execution of all the algorithms in all benchmark
  functions for all dimensions), \texttt{cec17.extended.final}, and
  the mean data set (which aggregates the results among the
  iterations), \texttt{cec17.final}.
- In most of the pairwise comparisons we have involved EBOwithCMAR
  and jSO algorithms, as they are the best classified algorithms in
  the competition.
- \autoref{tab:data-mean} shows the mean among different
  iterations of the results obtained at the end of all of the steps of
  each algorithm on each benchmark function for the 10 Dimension
  scenario.
- The tables included in this section are obtained with the function
  \texttt{AdjustFormatTable} of the package used, which is helpful to
  highlight the rejected hypotheses by emboldening the associated
  $p$-values.

# Parametric analysis


## Preliminars

Traditionally the statistical tests applied to the comparison of
different algorithms belonged to the parametric family of tests. We
start the statistical analysis of the results with these kinds of
tests and the study of the conditions in order to use them safely.

The traditional parametric test used in the context of a comparison of
multiple algorithms over multiple problems (benchmarks) is the ANOVA
test. This test makes some assumptions that should be checked before
it is performed:

- The distribution of the results for each algorithm among
  different benchmarks follows a Gaussian distribution.
- The standard deviation of results among groups is equal.


```{r shapiro, echo=TRUE, results="asis"}
shapiro.test <- split(cec17.final, cec17.final$Dimension) %>% 
   lapply(function(x) split(x, x$Algorithm) %>% 
   lapply(function(y) shapiro.test(y$Result)$p.value)) %>% 
   unlist() %>% matrix(ncol = 4) %>% as.data.frame()

AdjustFormatTable(shapiro.test, 
                  colnames = paste("Dim.", levels(cec17.final$Dimension)),
                  rownames = levels(cec17.final$Algorithm), print.code = TRUE,
                  digits = -2, type = "latex")
```    

In this table we gather the $p$-values associated with the normality
of each group of mean results for an algorithm in a dimension
scenario. All the null hypotheses are rejected because the $p$-values
are less than $0.05$, which means that we reject that the distribution
of the mean results for each benchmark function follow a normal
distribution. This conclusion could be expected because of the
different difficulty of the benchmark functions in higher dimension
scenarios. This is marked with boldface in subsequent tables.


In some circumstances like the Multi-Objective Optimization we need to
include different measures in the comparison. We will now consider the
results of the different dimensions as if they were different measures
in the same benchmark function. Then, in order to perform the
Hotelling's $T^2$ test we first check the normality of the population
with the multivariate generalization of Shapiro-Wilk's test. The
following table shows that the normality hypothesis is rejected for
every algorithm. Therefore, we stop the parametric analysis of the
results here because the assumptions of parametric tests are not
satisfied.


```{r mv.shapiro}
sapply(split(cec17.final, cec17.final$Algorithm), 
       function(x){
          test <- x %>%
            select(-Algorithm) %>% 
            tidyr::spread(Dimension, Result) %>%
 	    select(-Benchmark)  %>%
            as.matrix() %>%
            mvShapiroTest::mvShapiro.Test()
          return(test$p.value)
       }) %>% 
       matrix(ncol = 1, 
              dimnames = list(levels(cec17.final$Algorithm),
                              "p-value")) %>%
  AdjustFormatTable(print.code = T)
```



# Non-parametric tests


In this subsection we perform the most popular tests in the field of the
comparison of optimization algorithms. We continue using the aggregated
results across the different runs at the end of the iterations, except
for the Page test for the study of the convergence.

## Classic tests

### Pairwise comparison

First, we perform the non-parametric pairwise comparisons with the
Sign, Wilcoxon and Wilcoxon Rank-Sum tests for the 10 dimension
scenario. The hypotheses of the equality of the medians is only
rejected by the Wilcoxon Rank-Sum test, and we can see in the
following table how for example Wilcoxon's statistics $R+$ and $R-$
are similar, which means that there is no significant difference
between the ranking of the observations where one algorithm
outperforms the other. Then, the next step requires all the different
algorithms in the competition to be involved in the comparison.


```{r}
EBO <- unlist(select(filter(cec17.final, Algorithm == "EBO", Dimension == 10), Result), use.names = F)
jSO <- unlist(select(filter(cec17.final, Algorithm == "jSO", Dimension == 10), Result), use.names = F)
```


```{r}
sign.results <- rNPBST::binomialSign.test(cbind(EBO, jSO))
wilcoxon.results <- rNPBST::wilcoxon.test(cbind(EBO, jSO))
wilcoxonRS.results <- rNPBST::wilcoxonRankSum.test(cbind(EBO, jSO))

p.values <- sapply(list(sign.results,wilcoxon.results, wilcoxonRS.results), function(x) x$p.value[3])
lapply(list(sign.results,wilcoxon.results, wilcoxonRS.results), function(x) x$statistic)
```


```{r}
ApplyNPPairwise <- function(data, test){
split(data, data$Dimension) %>%
   lapply(function(x){
      y <- test(matrix(c(unlist(select(filter(x, Algorithm == "EBO"), Result)),
                                              unlist(select(filter(x, Algorithm == "jSO"), Result))),
                                            ncol = 2))$p.value[3]
   }) %>%
    unlist
}

results.sign <- ApplyNPPairwise(cec17.final, rNPBST::binomialSign.test)
results.wilcoxon <- ApplyNPPairwise(cec17.final, rNPBST::wilcoxon.test)
results.wilcoxonRS <- ApplyNPPairwise(cec17.final, rNPBST::wilcoxonRankSum.test)

results.nppairwise <- matrix(c(results.sign, results.wilcoxon,results.wilcoxonRS),
                             nrow = 3, byrow = T,
                             dimnames = list(c("Sign","Wilcoxon", "Wilcoxon Rank-Sum"), 
                                             paste("Dim.", levels(cec17.final$Dimension)))) %>%
    AdjustFormatTable(caption = "Non-parametric pairwise comparison between EBO and jSO", label = "tab:np-pairwise", print.code = T)
```

### Multiple comparison

```{r apply.np.test}
ApplyNPTest <- function(data, test){
    split(data, data$Dimension) %>%
    lapply(function(x)
        select(x, -Dimension) %>% 
        tidyr::spread(Algorithm, Result) %>%
        select(-Benchmark) %>%
        test %>% get("p.value", .)) %>%
    unlist
}
```

```{r np.test}
friedman.results <- ApplyNPTest(cec17.final, friedman.test)
friedman.ar.results <- ApplyNPTest(cec17.final, friedmanAR.test)
iman.davenport.results <- ApplyNPTest(cec17.final, imanDavenport.test)
quade.results <- split(cec17.final, cec17.final$Dimension) %>%
  lapply(function(x){
    x <- select(x, -Dimension)
    quade.test(x$Result, x$Algorithm, x$Benchmark) %>% get("p.value", .)
  }) %>%
  unlist(use.names = T)

non.parametric <- matrix(c(friedman.results, friedman.ar.results,
                           iman.davenport.results, quade.results),
                         byrow = T, ncol = 4)

AdjustFormatTable(non.parametric,
                  colnames = paste("Dim.", levels(cec17.final$Dimension)),
                  rownames = c("Friedman" ,"Friedman AR", "Iman-Davenport", "Quade"),
                  caption = "Non-Parametric tests",
                  label = "tab:np-tests")
```

We can see in this table how the tests that involve multiple
algorithms reject the null hypotheses, that is, the equivalence of the
medians of the results of the different benchmarks.  We must keep in
mind that a comparison between thirteen algorithms is not the
recommended procedure if we want to compare our proposal. We should
only include the state-of-the-art algorithms in the comparison,
because the inclusion of an algorithm with lower performance could
lead to the rejection of the null hypothesis, not due to the
differences between our algorithm and the comparison group, but
because of the differences between this dummy algorithm and the
others.

## Post-hoc tests

Then, we proceed to perform the post-hoc tests in order to determine
the location of the differences between these algorithms. We use the
modification of the classic non-parametric tests to obtain the
$p$-value associated with each hypothesis, although we should adjust
these $p$-values with a post-hoc procedure.

### Control algorithm 

To illustrate this, we first suppose that we are in a One versus all
scenario where we are presenting our algorithm (we will use
EBOwithCMAR, the winner of the CEC'17 competition). The possible
approach here, as in the rest of the analysis is:

- Considering all the results in the different dimensions as if they
  were different function or benchmarks, we would only obtain a single
  $p$-value for the comparison between EBO-CMAR with each contestant
  algorithm. The adjusted $p$-values are shown in the following table
  for the Friedman, Friedman Aligned-Rank and Quade test. Here we see
  that there is not much difference between the different tests and
  that differences are found in the comparison with DYYPO, IDEN, MOS,
  PPSO, RBI and TFL, although the sign of these differences need to be
  checked in the raw data.

```{r post.hoc}
cec17.final.10 <- filter(cec17.final, Dimension == 10)
ApplyNPTestPostHoc <- function(data, test, adjust, control = NULL){
    data %>%
        tidyr::spread(Algorithm, Result) %>%
        select(-Benchmark, -Dimension) %>%
        test(control = control) %>%
        adjust
}

results.post.friedman <- ApplyNPTestPostHoc(cec17.final, scmamp::friedmanPost,
                                           scmamp::adjustHolland, control = "EBO")
results.post.friedmanAR <- ApplyNPTestPostHoc(cec17.final, scmamp::friedmanPost,
                                             scmamp::adjustHolland, control = "EBO")
results.post.quade <- ApplyNPTestPostHoc(cec17.final, scmamp::quadePost, 
                                         scmamp::adjustHolland, control = "EBO")

results.post.global.holland <- rbind(results.post.friedman,
                                    results.post.friedmanAR,
                                    results.post.quade)

AdjustFormatTable(as.data.frame(t(results.post.global.holland)),
                  colnames = c("Friedman","FriedmanAR","Quade"),
                  caption = "Post-Hoc Non-Parametric tests with control algorithm",
                  label = "tab:np-tests-ph-control", print.code = T)
```

- If we wanted to show that the differences between the algorithms
  also persist in each group of results obtained across the different
  dimensions, we should perform these tests repeatedly and apply the
  appropiate post-hoc procedure later. In the following table we show
  the adjusted $p$-value obtained.

```{r post.hoc.grouped}
ApplyNPTestPostHoc.Grouped <- function(data, test, adjust, control = NULL){
  results <- split(data, data$Dimension) %>%
    sapply(function(x)
      select(x, -Dimension) %>% 
        tidyr::spread(Algorithm, Result) %>%
        select(-Benchmark) %>%
        test(control = control)) %>%
    adjust
  return(results)
}
post.hoc.grouped <- ApplyNPTestPostHoc.Grouped(cec17.final, 
                                               scmamp::friedmanPost, 
                                               scmamp::adjustHolland,
                                               control = "EBO")

AdjustFormatTable(post.hoc.grouped,
                  colnames = paste("Dim", colnames(post.hoc.grouped)),
                  rownames = levels(cec17.final$Algorithm),
                  caption = "Results grouped by dimension. Friedman test + Holland adjust.",
                  label = "tab:grp-friedman-holland")
```

### $n$ versus $n$ scenario

In the scenario of the statistical analysis of the results obtained
during a competition, we do not focus on the comparison of the results
of a single algorithm, rather we would make all the posible pairs, and
therfore we would not use the control algorithm.

```{r all.vs.all}
ApplyNPTestPostHoc(cec17.final.10,
                   scmamp::friedmanPost,
                   scmamp::adjustHolland)  %>%
  AdjustFormatTable(caption = "Results $n$ vs $n$. Friedman test + Holland adjust.",
                    label = "tab:nvsn-friedman-holland", print.code = F)
```

The results of the $n \times n$ comparison using a Friedman test and a
Post-Hoc Holland adjust of the $p$-values is shown in this
table. Here, we can see that there is not a single algorithm whose
equivalence with the rest of the algorithms is discarded (in the 10
dimension scenario). However, for a multiple comparison with a high
number of algorithms, like in the competition used as example, the
adjustment makes finding differences between the algorithms more
difficult. If we observe the results of the best classified algorithms
in the competition, like jSO and EBO, we see that there are
significant differences with algorithms like MOS, PPSO or RBI but this
difference is not significant for LSHADE variants or MM.

The CD plot associated with the scenario of an $n$ vs. $n$ comparison
and performed with the Nemenyi test provides an interesting
visualization fo the significance of the observed paired
differences. In the next figure, we show the results of this
comparison, where the differences between the group of the first
classified algorithms whose equivalence cannot be discarded includes
up to the RBI algorithm (7th classified in the 10 Dimension
scenario). This plot, with several overlapped groups that contain many
algorithms, shows that the differences are hard to identify in
algorithms that perform similarly. In the following code, the results
have been multiplied by -1 in order to preserve the order with respect
to the minimization objective, as the function was designed to be used
with a maximization intention.

```{r}
results.matrix.10 <- filter(cec17.final, Dimension == 10) %>% 
	tidyr::spread("Algorithm", "Result") %>% 
	dplyr::select(-Benchmark, -Dimension)
scmamp::plotCD(-results.matrix.10)
```

## Convergence test

For the comparison of the convergence of two algorithms, we use the
Page test with the mean result across the different runs for each
benchmark function of two algorithms. These results could be equally
extended using the convenient adjustments. It is relevant to note that
the algorithms EBO (first classified) and LSCNE (third classified) do
not provide their partial results.

The results in the following table show the comparison of the
convergence of the jSO and the LSSPA algorithms in the competition for
the 10 and 100 dimension scenario. Here, the null hypothesis of the
difference between LSSPA and jSO getting a positive trend cannot be
rejected in the 50 dimension scenario. In the 100 dimension scenario,
the test also detects an increasing trend in the ranks of the
difference between jSO and DYYPO.

```{r convergence}
convergence.comparison.lshade <- filter(cec17.mean,
                                Algorithm == "LSSPA",
                                Dimension == 10) %>%
    select(-Dimension)
convergence.comparison.jso <- filter(cec17.mean,
                                Algorithm == "jSO",
                                Dimension == 10) %>%
    select(-Dimension)

convergence.comparison <- rbind(convergence.comparison.jso,
                                convergence.comparison.lshade) %>%
  as.data.frame() %>% reshape2::melt(id.vars = c("Algorithm", "Benchmark"), value.name = "Result") %>%
  transform(Perc.Iterations = as.numeric(gsub("perc.completion.", "", variable))) %>% select(-variable)
```

```{r}
convergence.comparison.jso <- select(convergence.comparison.jso, -Benchmark, -Algorithm)
convergence.comparison.lshade <- select(convergence.comparison.lshade, -Benchmark, -Algorithm)
cat(rNPBST::htest2Tex(rNPBST::page.test(-convergence.comparison.lshade+convergence.comparison.jso)))
```


```{r}
convergence.comparison.jso <- filter(cec17.mean,
                                Algorithm == "jSO",
                                Dimension == 100) %>%
    select(-Algorithm, -Benchmark, -Dimension)
convergence.comparison.dyypo <- filter(cec17.mean,
                                Algorithm == "DYYPO",
                                Dimension == 100) %>%
    select(-Algorithm, -Benchmark, -Dimension)
rNPBST::page.test(convergence.comparison.jso - convergence.comparison.dyypo)
rNPBST::page.test(-convergence.comparison.jso + convergence.comparison.dyypo)
```


## Confidence Intervals and Confidence Curves

In this subsection we show the use of confidence intervals and
confidence curves in the comparison of optimization results. First, we
must advise that these comparisons only take care of two algorithms at
a time, and a post-hoc correction would be needed if the comparison
involved a greater number of algorithms, as we will see in the
following examples.

We perform the comparison of the final results of PPSO and jSO
algorithms for the 10 dimension scenario. 

As the 0 effect is not included in the non-parametric confidence
interval, the null hypothesis is then rejected. The interval is very
wide, so we have not much certainty the true location of the
parameter. If we only had done the Wilcoxon test, we would have
obtained the associated $p$-value, and the null hypothesis would also
be rejected, so the difference between the medians are detected with
both methods. If we look at the confidence curve, we can reject the
classic null hypothesis if the interval bounded by the intersections
of the horizontal line at the $\alpha$ value and the curve does not
contain 0. The confidence curve associated with the previous test is
plotted in \autoref{fig:conf-curve}, where we check that the null
hypothesis can be rejected.

```{r confidence.intervals}
conf.interval.PPSO <- filter(cec17.final,Algorithm == "PPSO",
                           Dimension == 10) %>%
    select(Result)
conf.interval.jSO <- filter(cec17.final,Algorithm == "jSO",
                           Dimension == 10) %>%
    select(Result)

rNPBST::NP.ApproximateConfidenceInterval(x = conf.interval.PPSO,
                                        y = conf.interval.jSO, 
                                        paired = T)
rNPBST::PlotConfidenceCurve(x = conf.interval.PPSO,
                            y = conf.interval.jSO, 
                            m = 50, paired = T)
```

As the 0 effect is not included in the Non-parametric confidence
interval, the null hypothesis is then rejected, although the interval
is very wide, so we have not much certainty about that. If we only had
done the Wilcoxon test, we would have obtained the associated
$p$-value, and the null hypothesis would be also rejected, so the
difference between the medians are detected with both methods. If we
look at the confidence curve, we can reject the classic null
hypothesis if the intersection of the vertical line at the 0 level and
the horizontal line at the $\alpha$ value remains under the curve. 

```{r confidence.curve}
conf.interval.matrix <- cbind(conf.interval.PPSO, conf.interval.jSO)
colnames(conf.interval.matrix) <- c("PPSO", "jSO")
cat(rNPBST::htest2Tex(rNPBST::wilcoxon.test(conf.interval.matrix)))
```  



```{r}
rNPBST::NP.ApproximateConfidenceInterval(x = conf.interval.matrix[ ,1],
                                         y = conf.interval.matrix[ ,2], paired = T)
PlotConfidenceCurve(x = conf.interval.matrix[ ,1],
                    y = conf.interval.matrix[ ,2], 
                    m = 50, paired = T) +
  ggplot2::ggtitle("PPSO vs jSO")
```

## Multiple measures test - GLRT

```{r multiple.measures.nonparametric}
matrix.EBO <-  filter(cec17.final, Algorithm == "EBO") %>%
    select(-Algorithm) %>% tidyr::spread(Dimension, Result) %>%
    select(-Benchmark)
matrix.jSO <-  filter(cec17.final, Algorithm == "jSO") %>%
    select(-Algorithm) %>% tidyr::spread(Dimension, Result) %>%
    select(-Benchmark)
rNPBST::multipleMeasuresGLRT(matrix.EBO, matrix.jSO) 
```

In order to perform the Non-Parametric Multiple Measures test, we
consider that the results of the different dimensions as if they were
different measures in the same benchmark function. We select the means
of the executions of the two best algorithms and reshape them into a
matrix with the results of each benchmark in the rows and the
different dimensions in the columns. Then we use the test to see which
hypothesis of dominance is the most probable and if we can state that
the probability of this dominance statement is significant. According
to the results, we obtain that the most observed dominance statement
in the third position (which corresponds to the number 2, 0010 in
binary) and the configuration $[<,<,>,<]$, it is, EBO-CMAR obtains a
better result for the dimensions 10, 30 and 100 while jSO obtains
better results in the 50 dimension scenario. However, the associated
null hypothesis which states that the mentioned configuration is no
more probable than the following one, obtain a $p$-value of 0.25, so
this hypothesis cannot be rejected. The second most probable
configuration is in the 15th position (or number 14) and the
configuration $[>,>,>,<]$, which means that EBO-CMAR obtains worse
results in dimensions 10,30 and 50, so we cannot assure which is the
most probable situation in the lower dimension scenarios.

# Bayesian tests

In this section we illustrate the use of the proposed Bayesian
tests. The considerations are analogous to the ones made in the
frequentist case, as the described tests use the aggregations of the
runs to compare the results of the different benchmark functions, or
use these runs with the drawback of obtaining a restrained statement
about the results in one single problem.

## Bayesian Friedman test

We start with the Bayesian version of the Friedman test. In this test
we do not obtain a single $p$-value, but the accepted hypothesis. Due
to the high number of contestant algorithms and the memory needed to
allocate the covariance matrix of all the possible permutations, we
perform here the imprecise version of the test. The null hypothesis of
the equivalence of the mean ranks for the 10 dimension scenario is
rejected.

```{r bayesian.friedman.test}
dim.10 <- filter(cec17.final, Dimension == 10) %>%
    tidyr::spread(Algorithm, Result) %>% select(-Benchmark, -Dimension)

bft.results <- bayesianFriedman.test(dim.10, imprecise = TRUE, n.samples = 200)
bft.results$h
meanranks <- bft.results$meanranks
names(meanranks) <- colnames(bft.results$covariance)
xtable::xtable(as.matrix(meanranks))
```

## Bayesian Sign and Signed-Rank test

The originial proposal of the use of the Bayesian Sign and Signed-Rank
tests are destinated to the comparison of classification algorithms
and the proposed region of practical equivalence is $[-0.01,0.01]$ for
a measure in the range $[0,1]$. In the scenario of optimization
problems, we should be concern that the possible outcomes are
lower-bounded by 0 but in many functions there is not an upper bound
or the maximum is very high so we must follow another approach. As the
difference in the 30 dimension comparison is between 0 and 1527, we
state that the region of practical equivalence is $[-10,10]$.


```{r bayesian.pair.test}
jso <- filter(cec17.final, Algorithm == "jSO", Dimension == 10) %>%
    select(Result) %>% unlist()
ebo <- filter(cec17.final, Algorithm == "EBO", Dimension == 10) %>%
    select(Result) %>% unlist()

bst.results <- rNPBST::bayesianSign.test(ebo, jso,
                                       rope.min = -10, rope.max = 10)
plot(bst.results, num.points = 10000) + ggplot2::labs(x = "jSO", z = "EBO")

bsrt.results <- rNPBST::bayesianSignedRank.test(ebo, jso,
                                              rope.min = -10, rope.max = 10)
plot(bsrt.results, num.points = 10000)+ ggplot2::labs(x = "jSO", z = "EBO", y = "rope")
```

The test compute the probability of the true location of
$\textrm{EBO-CMAR} - \textrm{jSO}$ with respect to $0$, so both tests'
results gives that the left region (jSO obtain worse results) a
greater probability, although this is not significative. Rope
probability is low in comparison with the right and left region, and
this is because, although the number of wins and losses is similar for
both algorithms, the number of benchmark functions where the results
are quite similar is lower.

We can see the posterior probability density of the parameter in the
following plots, where each point represents an estimation of the
probability of the parameter to belong to each region of
interest. This means that we have repeatedly obtained the triplets of
the probability of each region to be the true location of the
difference between the two samples and then we have plotted these
triplets to obtain the posterior distribution of the parameter. If we
compare these results with a paired Wilcoxon test we see that the null
hypothesis of the equivalence of the means cannot be
rejected. However, using the Bayesian paradigm we cannot establish the
dominance of one algorithm over the other either, but while in the
frequentist paradigm we could be tempted to (erroneously) establish
that there is no difference between these algorithms with these plots
we obtain valuable information and we can see that here is not the same
situation, as the probability of the rope is very low.


```{r plot.pair.bayesian.tests}
wilcox.test(jso, ebo.cmar, paired = T)
plot(bst.results, num.points = 10000) 
plot(bsrt.results, num.points = 10000)
```

## Imprecise Dirichlet Process Test

The Imprecise Dirichlet Process consists in a more complex idea of the
previous tests although the implications of the use of the Bayesian
Tests could be clearer. In this test we try to not to introduce any
prior distribution, not even the prior distribution where both
algorithms have the same performance, but all the possible probability
measures $\alpha$, and then obtain an upper and a lower bounds for the
probability in which we are interested. The input consits in the
aggregated results among the different runs for all the benchmark
functions for a single dimension. The other parameters of the function
are the strong parameter $s$ and the pseudo-observation $c$. With
these data, we obtain two bounds for the posterior probability of the
first algorithm outperforming the second one, i.e. the probability of
$P(X \leq Y) \geq 0.5$. These are the possible scenarios:

- Both bounds are greater than 0.95: Then we can say that the first
  algorithm outperforms the second algorithm with a 95\% of
  probability.
- Both bounds are lower than 0.05: This is the inverse case. In this
  situation, the second algorithm outperforms the first algorithm with
  a 95\% of probability.
- Both bounds are between 0.05 and 0.95: Then we can say that the
  probability of one algorithm outperforming the other is lower than
  the desired probability of 0.95.
- Finally, if only one of the bounds is greater than 0.95 or lower
  than 0.05, the situation is indetermined and we cannot decide.

```{r bayesian.imprecise}
library(dplyr)
library(rNPBST)
jso <- filter(cec17.final, Algorithm == "jSO", Dimension == 50) %>%
    select(Result) %>% unlist()
ebo.cmar <- filter(cec17.final, Algorithm == "EBO", Dimension == 50) %>%
    select(Result) %>% unlist()

bayesian.imprecise.results <- bayesian.imprecise(jso, ebo.cmar)
plot(bayesian.imprecise.results) + ggplot2::ggtitle("JSO vs. EBO-CMAR")
```

According to the results of the Bayesian Imprecise Dirichlet Process,
the probability $\mathcal{P}[P(EBO-CMAR \leq jSO) \geq 0.5]$, that is
the probability of EBO-CMAR outperforming jSO, is between 0.92 and
0.96, so there is an indetermination and we cannot say that there is 
a probability greater than 0.95 of EBO-CMAR outperforming jSO, 
nor there is not such probability. These numbers represent
the area under the curve of the upper and lower distributions when
$P(X \leq Y) \geq 0.5$



## Bayesian Multiple Measures Test

In the Bayesian version of the Multiple Measures test the results are
analogous to the frequentist version. The most probable configuration
is also the dominance of the EBO-CMAR in the 10,30 and 100 dimensions
while the algorithm jSO obtains better results in the 50 dimension
according to this test, but the posterior probability is 0.841, so we
cannot say that the difference with respect the remaining
configurations is determinative.


```{r bayesian.multiple}
bayesianmultipleconditions.results <- rNPBST::bayesianMultipleConditions.test(a,b)
bayesianmultipleconditions.results$probabilities
```

## PlackettLuce

```{r}
library(rNPBST)
cec17.extended.10 <- filter(cec17.extended, Dimension == "10") %>% select(-Dimension)
a <- merge(cec17.extended.10, cec17.extended.10, by = c("Benchmark","Iteration")) %>%
  filter(Algorithm.x != Algorithm.y) %>%
  filter_at(vars(ends_with(".x"),
                 ends_with(".y")), 
            any_vars(. == 0)) %>%
  group_by(Algorithm.x, Algorithm.y) %>%
  mutate(End.x = sum(!!any_vars(. == 0)))
  summarise(r_ij = n(),
            w_ij = sum(vars(ends_with(".x")), any_vars(. == 0)))) %>%
  ungroup()
```

# Appendix

```{r}
base.data <- cec17.final %>%
  filter(Dimension == 10) %>%
  select(-Dimension) %>%
  reshape(idvar = "Benchmark", timevar = "Algorithm", direction = "wide") %>%
  transform(Benchmark = as.integer(levels(Benchmark))[Benchmark]) %>% arrange(Benchmark)
colnames(base.data)[-1] <- levels(cec17.final$Algorithm)
knitr::kable(base.data)
```
