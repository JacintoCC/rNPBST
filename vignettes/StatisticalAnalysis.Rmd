---
title: "Statistical Analysis"
output: rmarkdown::html_vignette
resource_files:
  - img/cd-plot-dim10.pdf
  - img/cd-plot-dim30.pdf
  - img/cd-plot-dim50.pdf
  - img/cd-plot-dim100.pdf
  - img/bayesian-multiple-dim10-eps-converted-to.pdf
  - img/bayesian-multiple-dim30-eps-converted-to.pdf
  - img/bayesian-multiple-dim50-eps-converted-to.pdf
  - img/bayesian-multiple-dim100-eps-converted-to.pdf
vignette: >
  %\VignetteIndexEntry{Statistical Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, echo = FALSE, error= TRUE)
```


```{r load.libraries, message=FALSE, warning=FALSE}
library(rNPBST)
library(dplyr)
library(scmamp)
library(xtable)
library(ggplot2)
```

This vignette comprehends a statistical analysis of the case of study
of the CECâ€™17 Special Session and Competition on Single Objective Real
Parameter Numerical Optimization can be illustrated. The results have
been included in the package as a dataset.

- We will mainly use two data sets, one with all the results (all
  iterations of the execution of all the algorithms in all benchmark
  functions for all dimensions), \texttt{cec17.extended.final}, and
  the mean data set (which aggregates the results among the
  iterations), \texttt{cec17.final}.
- In most of the pairwise comparisons we have involved EBOwithCMAR
  and jSO algorithms, as they are the best classified algorithms in
  the competition.
- The used data shows the mean among different iterations of the
  results obtained at the end of all of the steps of each algorithm on
  each benchmark function for the 10 Dimension scenario.
- The tables included in this section are obtained with the function
  \texttt{AdjustFormatTable} of the package used, which is helpful to
  highlight the rejected hypotheses by emboldening the associated
  $p$-values.

# Parametric analysis


## Preliminars

Traditionally the statistical tests applied to the comparison of
different algorithms belonged to the parametric family of tests. We
start the statistical analysis of the results with these kinds of
tests and the study of the conditions in order to use them safely.

The traditional parametric test used in the context of a comparison of
multiple algorithms over multiple problems (benchmarks) is the ANOVA
test. This test makes some assumptions that should be checked before
it is performed:

- The distribution of the results for each algorithm among
  different benchmarks follows a Gaussian distribution.
- The standard deviation of results among groups is equal.


```{r shapiro, echo=TRUE, results="asis"}
shapiro.test <- split(cec17.final, cec17.final$Dimension) %>% 
   lapply(function(x) split(x, x$Algorithm) %>% 
   lapply(function(y) shapiro.test(y$Result)$p.value)) %>% 
   unlist() %>% matrix(ncol = 4) %>% as.data.frame()

AdjustFormatTable(shapiro.test, 
                  colnames = paste("Dim.", levels(cec17.final$Dimension)),
                  rownames = levels(cec17.final$Algorithm), print.code = FALSE,
                  digits = -2)
```

In this table we gather the $p$-values associated with the normality
of each group of mean results for an algorithm in a dimension
scenario. All the null hypotheses are rejected because the $p$-values
are less than $0.05$, which means that we reject that the distribution
of the mean results for each benchmark function follow a normal
distribution. This conclusion could be expected because of the
different difficulty of the benchmark functions in higher dimension
scenarios. This is marked with boldface in subsequent tables.


In some circumstances like the Multi-Objective Optimization we need to
include different measures in the comparison. We will now consider the
results of the different dimensions as if they were different measures
in the same benchmark function. Then, in order to perform the
Hotelling's $T^2$ test we first check the normality of the population
with the multivariate generalization of Shapiro-Wilk's test. The
following table shows that the normality hypothesis is rejected for
every algorithm. Therefore, we stop the parametric analysis of the
results here because the assumptions of parametric tests are not
satisfied.


```{r mv.shapiro}
sapply(split(cec17.final, cec17.final$Algorithm), 
       function(x){
          test <- x %>%
             select(-Algorithm) %>% 
             tidyr::spread(Dimension, Result) %>%
             select(-Benchmark)  %>%
             as.matrix() %>%
             mvShapiroTest::mvShapiro.Test()
          return(test$p.value)
       }) %>% 
   matrix(ncol = 1, 
          dimnames = list(levels(cec17.final$Algorithm),
                          "p-value")) %>%
   AdjustFormatTable(print.code = FALSE)
```

# Non-parametric tests


In this subsection we perform the most popular tests in the field of the
comparison of optimization algorithms. We continue using the aggregated
results across the different runs at the end of the iterations, except
for the Page test for the study of the convergence.

## Classic tests

### Pairwise comparison

First, we perform the non-parametric pairwise comparisons with the
Sign, Wilcoxon and Wilcoxon Rank-Sum tests for the 10 dimension
scenario. The hypotheses of the equality of the medians is only
rejected by the Wilcoxon Rank-Sum test, and we can see in the
following table how for example Wilcoxon's statistics $R+$ and $R-$
are similar, which means that there is no significant difference
between the ranking of the observations where one algorithm
outperforms the other. Then, the next step requires all the different
algorithms in the competition to be involved in the comparison.


```{r}
EBO <- unlist(select(filter(cec17.final, Algorithm == "EBO", Dimension == 10), Result), use.names = F)
jSO <- unlist(select(filter(cec17.final, Algorithm == "jSO", Dimension == 10), Result), use.names = F)
```

```{r}
sign.results <- rNPBST::binomialSign.test(cbind(EBO, jSO))
wilcoxon.results <- rNPBST::wilcoxon.test(cbind(EBO, jSO))
wilcoxonRS.results <- rNPBST::wilcoxonRankSum.test(cbind(EBO, jSO))

p.values <- sapply(list(sign.results,wilcoxon.results, wilcoxonRS.results), function(x) x$p.value[3])
```


```{r}
ApplyNPPairwise <- function(data, test){
split(data, data$Dimension) %>%
   lapply(function(x){
      y <- test(matrix(c(unlist(select(filter(x, Algorithm == "EBO"), Result)),
                                              unlist(select(filter(x, Algorithm == "jSO"), Result))),
                                            ncol = 2))$p.value[3]
   }) %>%
    unlist
}

results.sign <- ApplyNPPairwise(cec17.final, rNPBST::binomialSign.test)
results.wilcoxon <- ApplyNPPairwise(cec17.final, rNPBST::wilcoxon.test)
results.wilcoxonRS <- ApplyNPPairwise(cec17.final, rNPBST::wilcoxonRankSum.test)

results.nppairwise <- matrix(c(results.sign, results.wilcoxon,results.wilcoxonRS),
                             nrow = 3, byrow = T,
                             dimnames = list(c("Sign","Wilcoxon", "Wilcoxon Rank-Sum"), 
                                             paste("Dim.", levels(cec17.final$Dimension)))) %>%
    AdjustFormatTable(caption = "Non-parametric pairwise comparison between EBO and jSO",
                      label = "tab:np-pairwise", print.code = FALSE)
```

### Multiple comparison

```{r apply.np.test}
ApplyNPTest <- function(data, test){
    split(data, data$Dimension) %>%
    lapply(function(x)
        select(x, -Dimension) %>% 
        tidyr::spread(Algorithm, Result) %>%
        select(-Benchmark) %>%
        test %>% get("p.value", .)) %>%
    unlist
}
```

```{r np.test}
friedman.results <- ApplyNPTest(cec17.final, friedman.test)
friedman.ar.results <- ApplyNPTest(cec17.final, friedmanAR.test)
iman.davenport.results <- ApplyNPTest(cec17.final, imanDavenport.test)
quade.results <- split(cec17.final, cec17.final$Dimension) %>%
  lapply(function(x){
    x <- select(x, -Dimension)
    quade.test(x$Result, x$Algorithm, x$Benchmark) %>% get("p.value", .)
  }) %>%
  unlist(use.names = T)

non.parametric <- matrix(c(friedman.results, friedman.ar.results,
                           iman.davenport.results, quade.results),
                         byrow = T, ncol = 4)

AdjustFormatTable(non.parametric,
                  colnames = paste("Dim.", levels(cec17.final$Dimension)),
                  rownames = c("Friedman" ,"Friedman AR", "Iman-Davenport", "Quade"),
                  caption = "Non-Parametric tests",
                  label = "tab:np-tests", print.code = FALSE)
```

We can see in this table how the tests that involve multiple
algorithms reject the null hypotheses, that is, the equivalence of the
medians of the results of the different benchmarks.  We must keep in
mind that a comparison between thirteen algorithms is not the
recommended procedure if we want to compare our proposal. We should
only include the state-of-the-art algorithms in the comparison,
because the inclusion of an algorithm with lower performance could
lead to the rejection of the null hypothesis, not due to the
differences between our algorithm and the comparison group, but
because of the differences between this dummy algorithm and the
others.

## Post-hoc tests

Then, we proceed to perform the post-hoc tests in order to determine
the location of the differences between these algorithms. We use the
modification of the classic non-parametric tests to obtain the
$p$-value associated with each hypothesis, although we should adjust
these $p$-values with a post-hoc procedure.

### Control algorithm 

To illustrate this, we first suppose that we are in a One versus all
scenario where we are presenting our algorithm (we will use
EBOwithCMAR, the winner of the CEC'17 competition). The possible
approach here, as in the rest of the analysis is:

- Considering all the results in the different dimensions as if they
  were different function or benchmarks, we would only obtain a single
  $p$-value for the comparison between EBO-CMAR with each contestant
  algorithm. The adjusted $p$-values are shown in the following table
  for the Friedman, Friedman Aligned-Rank and Quade test. Here we see
  that there is not much difference between the different tests and
  that differences are found in the comparison with DYYPO, IDEN, MOS,
  PPSO, RBI and TFL, although the sign of these differences need to be
  checked in the raw data.

```{r post.hoc}
cec17.final.10 <- filter(cec17.final, Dimension == 10)
ApplyNPTestPostHoc <- function(data, test, adjust, control = NULL){
    data %>%
        tidyr::spread(Algorithm, Result) %>%
        select(-Benchmark, -Dimension) %>%
        test(control = control) %>%
        adjust
}

results.post.friedman <- ApplyNPTestPostHoc(cec17.final, scmamp::friedmanPost,
                                           scmamp::adjustHolland, control = "EBO")
results.post.friedmanAR <- ApplyNPTestPostHoc(cec17.final, scmamp::friedmanPost,
                                             scmamp::adjustHolland, control = "EBO")
results.post.quade <- ApplyNPTestPostHoc(cec17.final, scmamp::quadePost, 
                                         scmamp::adjustHolland, control = "EBO")

results.post.global.holland <- rbind(results.post.friedman,
                                    results.post.friedmanAR,
                                    results.post.quade) %>% t
colnames(results.post.global.holland) <- c("Friedman","FriedmanAR","Quade") 

# knitr::kable(results.post.global.holland)
AdjustFormatTable(results.post.global.holland, print.code = FALSE)
```

- If we wanted to show that the differences between the algorithms
  also persist in each group of results obtained across the different
  dimensions, we should perform these tests repeatedly and apply the
  appropiate post-hoc procedure later. In the following table we show
  the adjusted $p$-value obtained.

```{r post.hoc.grouped}
ApplyNPTestPostHoc.Grouped <- function(data, test, adjust, control = NULL){
  results <- split(data, data$Dimension) %>%
    sapply(function(x)
      select(x, -Dimension) %>% 
        tidyr::spread(Algorithm, Result) %>%
        select(-Benchmark) %>%
        test(control = control)) %>%
    adjust
  return(results)
}
post.hoc.grouped <- ApplyNPTestPostHoc.Grouped(cec17.final, 
                                               scmamp::friedmanPost, 
                                               scmamp::adjustHolland,
                                               control = "EBO")

AdjustFormatTable(post.hoc.grouped, print.code = FALSE,
                  colnames = paste("Dim", colnames(post.hoc.grouped)),
                  rownames = levels(cec17.final$Algorithm),
                  caption = "Results grouped by dimension. Friedman test + Holland adjust.",
                  label = "tab:grp-friedman-holland")
```

### $n$ versus $n$ scenario

In the scenario of the statistical analysis of the results obtained
during a competition, we do not focus on the comparison of the results
of a single algorithm, rather we would make all the posible pairs, and
therfore we would not use the control algorithm.

```{r all.vs.all}
ApplyNPTestPostHoc(cec17.final.10,
                   scmamp::friedmanPost,
                   scmamp::adjustHolland)  %>%
  AdjustFormatTable(caption = "Results $n$ vs $n$. Friedman test + Holland adjust.",
                    label = "tab:nvsn-friedman-holland", print.code = FALSE)
```

The results of the $n \times n$ comparison using a Friedman test and a
Post-Hoc Holland adjust of the $p$-values is shown in this
table. Here, we can see that there is not a single algorithm whose
equivalence with the rest of the algorithms is discarded (in the 10
dimension scenario). However, for a multiple comparison with a high
number of algorithms, like in the competition used as example, the
adjustment makes finding differences between the algorithms more
difficult. If we observe the results of the best classified algorithms
in the competition, like jSO and EBO, we see that there are
significant differences with algorithms like MOS, PPSO or RBI but this
difference is not significant for LSHADE variants or MM.

The CD plot associated with the scenario of an $n$ vs. $n$ comparison
and performed with the Nemenyi test provides an interesting
visualization fo the significance of the observed paired
differences. In the next figure, we show the results of this
comparison, where the differences between the group of the first
classified algorithms whose equivalence cannot be discarded includes
up to the RBI algorithm (7th classified in the 10 Dimension
scenario). This plot, with several overlapped groups that contain many
algorithms, shows that the differences are hard to identify in
algorithms that perform similarly. In the following code, the results
have been multiplied by -1 in order to preserve the order with respect
to the minimization objective, as the function was designed to be used
with a maximization intention.

```{r}
results.matrix.10 <- filter(cec17.final, Dimension == 10, Algorithm != "MOS12") %>% 
	tidyr::spread("Algorithm", "Result") %>% 
	dplyr::select(-Benchmark, -Dimension)
scmamp::plotCD(-results.matrix.10)
```

```{r}
results.matrix.30 <- filter(cec17.final, Dimension == 30, Algorithm != "MOS12") %>% 
	tidyr::spread("Algorithm", "Result") %>% 
	dplyr::select(-Benchmark, -Dimension)
scmamp::plotCD(-results.matrix.30)
```


```{r}
results.matrix.50 <- filter(cec17.final, Dimension == 50, Algorithm != "MOS12") %>% 
	tidyr::spread("Algorithm", "Result") %>% 
	dplyr::select(-Benchmark, -Dimension)
scmamp::plotCD(-results.matrix.50)
```


```{r}
results.matrix.100 <- filter(cec17.final, Dimension == 100, Algorithm != "MOS12") %>% 
	tidyr::spread("Algorithm", "Result") %>% 
	dplyr::select(-Benchmark, -Dimension)
scmamp::plotCD(-results.matrix.100)
```

## Convergence test

For the comparison of the convergence of two algorithms, we use the
Page test with the mean result across the different runs for each
benchmark function of two algorithms. These results could be equally
extended using the convenient adjustments. It is relevant to note that
the algorithms EBO (first classified) and LSCNE (third classified) do
not provide their partial results.

The results in the following table show the comparison of the
convergence of the jSO and the LSSPA algorithms in the competition for
the 10 and 100 dimension scenario. Here, the null hypothesis of the
difference between LSSPA and jSO getting a positive trend cannot be
rejected in the 50 dimension scenario. In the 100 dimension scenario,
the test also detects an increasing trend in the ranks of the
difference between jSO and DYYPO.

```{r convergence}
convergence.comparison.lshade <- filter(cec17.mean,
                                Algorithm == "LSSPA",
                                Dimension == 10) %>%
    select(-Dimension)
convergence.comparison.jso <- filter(cec17.mean,
                                Algorithm == "jSO",
                                Dimension == 10) %>%
    select(-Dimension)

convergence.comparison <- rbind(convergence.comparison.jso,
                                convergence.comparison.lshade) %>%
  as.data.frame() %>% reshape2::melt(id.vars = c("Algorithm", "Benchmark"), value.name = "Result") %>%
  transform(Perc.Iterations = as.numeric(gsub("perc.completion.", "", variable))) %>% select(-variable) 

convergence.comparison.jso <- select(convergence.comparison.jso, -Benchmark, -Algorithm)
convergence.comparison.lshade <- select(convergence.comparison.lshade, -Benchmark, -Algorithm)
page.results <- rNPBST::page.test(-convergence.comparison.jso + convergence.comparison.lshade)
page.results %>% htest2Tex() %>% cat()
```


```{r}
convergence.comparison.jso <- filter(cec17.mean,
                                Algorithm == "jSO",
                                Dimension == 100) %>%
    select(-Algorithm, -Benchmark, -Dimension)
convergence.comparison.dyypo <- filter(cec17.mean,
                                Algorithm == "DYYPO",
                                Dimension == 100) %>%
    select(-Algorithm, -Benchmark, -Dimension)
page.results <- rNPBST::page.test(-convergence.comparison.jso + convergence.comparison.dyypo)
page.results
```

## Confidence Intervals and Confidence Curves

In this subsection we show the use of confidence intervals and
confidence curves in the comparison of optimization results. First, we
must advise that these comparisons only take care of two algorithms at
a time, and a post-hoc correction would be needed if the comparison
involved a greater number of algorithms, as we will see in the
following examples.

We perform the comparison of the final results of PPSO and jSO
algorithms for the 10 dimension scenario. 

As the 0 effect is not included in the non-parametric confidence
interval, the null hypothesis is then rejected. The interval is very
wide, so we have not much certainty the true location of the
parameter. If we only had done the Wilcoxon test, we would have
obtained the associated $p$-value, and the null hypothesis would also
be rejected, so the difference between the medians are detected with
both methods. If we look at the confidence curve, we can reject the
classic null hypothesis if the interval bounded by the intersections
of the horizontal line at the $\alpha$ value and the curve does not
contain 0. The confidence curve associated with the previous test is
plotted in the following figure, where we check that the null
hypothesis can be rejected.

```{r confidence.intervals}
conf.interval.PPSO <- filter(cec17.final,Algorithm == "PPSO",
                           Dimension == 10) %>%
    select(Result)
conf.interval.jSO <- filter(cec17.final,Algorithm == "jSO",
                           Dimension == 10) %>%
    select(Result)

np.ci.results <- rNPBST::NP.ApproximateConfidenceInterval(x = conf.interval.PPSO[,1],
                                                          y = conf.interval.jSO[,1], 
                                                          paired = T)

rNPBST::PlotConfidenceCurve(x = conf.interval.PPSO[,1],
                            y = conf.interval.jSO[,1], 
                            m = 50, paired = T)
```

As the 0 effect is not included in the Non-parametric confidence
interval, the null hypothesis is then rejected, although the interval
is very wide, so we have not much certainty about that. If we only had
done the Wilcoxon test, we would have obtained the associated
$p$-value, and the null hypothesis would be also rejected, so the
difference between the medians are detected with both methods. If we
look at the confidence curve, we can reject the classic null
hypothesis if the intersection of the vertical line at the 0 level and
the horizontal line at the $\alpha$ value remains under the curve. 

```{r confidence.curve}
conf.interval.matrix <- cbind(conf.interval.PPSO, conf.interval.jSO)
colnames(conf.interval.matrix) <- c("PPSO", "jSO")
rNPBST::wilcoxon.test(conf.interval.matrix) %>%
   htest2Tex() %>%
   cat()
```

```{r}
rNPBST::NP.ApproximateConfidenceInterval(x = conf.interval.matrix[ ,1],
                                         y = conf.interval.matrix[ ,2], paired = T)
PlotConfidenceCurve(x = conf.interval.matrix[ ,1],
                    y = conf.interval.matrix[ ,2], 
                    m = 50, paired = T) +
  ggplot2::ggtitle("PPSO vs jSO")
```

## Multiple measures test - GLRT

```{r multiple.measures.nonparametric}
matrix.EBO <-  filter(cec17.final, Algorithm == "EBO") %>%
    select(-Algorithm) %>% tidyr::spread(Dimension, Result) %>%
    select(-Benchmark)
matrix.jSO <-  filter(cec17.final, Algorithm == "jSO") %>%
    select(-Algorithm) %>% tidyr::spread(Dimension, Result) %>%
    select(-Benchmark)

glrt.results <- rNPBST::multipleMeasuresGLRT(matrix.EBO, matrix.jSO)
knitr::kable(glrt.results$statistic$n.vector)
```

In some circumstances like the Multi-Objective Optimization we need to
include different measures in the comparison.  This is the scenario of
application of the Non-Parametric Multiple Measures test. We select
the means of the executions of the two best algorithms and reshape
them into a matrix with the results of each benchmark in the rows and
the different dimensions in the columns. Then we use the test to see
which hypothesis of dominance is the most probable and if we can state
that the probability of this dominance statement is
significant. According to the results shown in this table, we obtain
that the most observed dominance statement is the configuration
$[<,<,>,<]$, it is, EBO-CMAR obtains a better result for the
dimensions 10, 30 and 100 while jSO obtains better results in the 50
dimension scenario. However, the associated null hypothesis which
states that the mentioned configuration is no more probable than the
following one, obtain a $p$-value of 0.25, so this hypothesis cannot
be rejected. The second most probable configuration is $[>,>,>,<]$,
which means that EBO-CMAR obtains worse results in dimensions 10,30
and 50, so we cannot be certain which is the most probable situation
in the lower dimension scenarios. It is relevant to note that the
number of observations can be a real value as the weight of an
observation is divided between the possible configuration when there
is a tie for any measure.

# Bayesian tests

In this section we illustrate the use of the described Bayesian tests.
The considerations are analogous to the ones made in the frequentist
case, as the described tests use the aggregations of the runs to compare
the results of the different benchmark functions, or use these runs with
the drawback of obtaining a restrained statement about the results in
one single problem.

## Bayesian Friedman test

We start with the Bayesian version of the Friedman test. In this test
we do not obtain a single $p$-value, but the accepted hypothesis. Due
to the high number of contestant algorithms and the memory needed to
allocate the covariance matrix of all the possible permutations, here
we will perform the imprecise version of the test. The null hypothesis
of the equivalence of the mean ranks for the 10 dimension scenario is
rejected. 

```{r bayesian.friedman.test}
spread.final.10 <- tidyr::spread(cec17.final.10, Algorithm, Result) %>% 
	select(-Benchmark, -Dimension)
bft.results <- bayesianFriedman.test(spread.final.10, imprecise = TRUE, 
                                    n.samples = 200)
```

## Bayesian Sign and Signed-Rank test

The originial proposal of the use of the Bayesian Sign and Signed-Rank
tests is the comparison of classification algorithms and the proposed
\textit{rope} is $[-0.01,0.01]$ for a measure in the range $[0,1]$. In
the scenario of optimization problems, we should be concerned that the
possible outcomes are lower-bounded by 0 but in many functions there
is not an upper bound or the maximum is very high, so we must follow
another approach. As the difference in the 30 dimension comparison is
between 0 and 1527, we state that the region of practical equivalence
is $[-10,10]$.

The tests compute the probability of the true location of
$\textrm{EBO-CMAR} - \textrm{jSO}$ with respect to $0$, so both tests'
results shows that the left region (jSO obtain worse results) results
in greater probability, although this is not significant. Rope
probability is low in comparison with the right and left region, and
this is because, although the number of wins and losses is similar for
both algorithms, the number of benchmark functions where the results
are quite similar is lower.

We can see the posterior probability density of the parameter in the
following figures, where each point represents an estimation of the
probability of the parameter which belongs to each region of
interest. The proportion of the location of the points is shown in the
associated table. This means that we have repeatedly obtained the
triplets of the probability of each region to be the true location of
the difference between the two samples, and then we have plotted these
triplets to obtain the posterior distribution of the parameter. If we
compare these results with a paired Wilcoxon test, we see that the
null hypothesis of the equivalence of the means cannot be rejected.

However, using the Bayesian paradigm we cannot establish the dominance
of one algorithm over the other either, while in the frequentist
paradigm we could be tempted to (erroneously) establish that there is
no difference between these algorithms. With these plots we can see
this is not the same situation, as the probability of the rope is very
low.


```{r bayesian.pair.test, out.width = "400px"}
jso <- filter(cec17.final, Algorithm == "jSO", Dimension == 100) %>%
    select(Result) %>% unlist()
ebo <- filter(cec17.final, Algorithm == "EBO", Dimension == 100) %>%
    select(Result) %>% unlist()

bst.results <- rNPBST::bayesianSign.test(ebo, jso,
                                       rope.min = -10, rope.max = 10)
plot(bst.results, num.points = 10000) +
    ggplot2::labs(x = "jSO", z = "EBO") + 
   theme(text = element_text(size=18))

```


```{r, out.width = "400px"}
jso <- filter(cec17.final, Algorithm == "jSO", Dimension == 100) %>%
    select(Result) %>% unlist()
ebo <- filter(cec17.final, Algorithm == "EBO", Dimension == 100) %>%
    select(Result) %>% unlist()

bsrt.results <- rNPBST::bayesianSignedRank.test(ebo, jso,
                                              rope.min = -10, rope.max = 10)
plot(bsrt.results, num.points = 10000) +
    ggplot2::labs(x = "jSO", z = "EBO", y = "rope") +
   theme(text = element_text(size=18))

```


## Imprecise Dirichlet Process Test

The Imprecise Dirichlet Process consists in a more complex idea of the
previous tests although the implications of the use of the Bayesian
Tests could be clearer. In this test we try to not introduce any prior
distribution, not even the prior distribution where both algorithms
have the same performance, but all the possible probability measures
$\alpha$, and then obtain an upper and a lower bounds for the
probability in which we are interested. The input consists of the
aggregated results among the different runs for all the benchmark
functions for a single dimension. The other parameters of the function
are the strong parameter $s$ and the pseudo-observation $c$. With
these data, we obtain two bounds for the posterior probability of the
first algorithm outperforming the second one, i.e.~the probability of
$P(X \leq Y) \geq 0.5$. These are the possible scenarios:

- Both bounds are greater than 0.95: Then we can say that the first
  algorithm outperforms the second algorithm with 95\% probability.
- Both bounds are lower than 0.05: This is the inverse case. In
  this situation, the second algorithm outperforms the first algorithm
  with 95\% probability.
- Both bounds are between 0.05 and 0.95: Then we can say that the
  probability of one algorithm outperforming the other is lower than
  the desired probability of 0.95.
- Finally, if only one of the bounds is greater than 0.95 or lower
  than 0.05, the situation is undetermined and we cannot decide.

```{r bayesian.imprecise, out.width = "400px"}
bayesian.imprecise.results <- bayesian.imprecise(jso, ebo)
plot(bayesian.imprecise.results) + ggplot2::ggtitle("JSO vs. EBO-CMAR")
```

According to the results of the Bayesian Imprecise Dirichlet Process
test, the probability under the Dirichlet Process of $P(EBO \leq jSO)
\geq 0.5$, that is the probability of EBO-CMAR outperforming jSO, is
between 0.92 and 0.96, so there is an indetermination, and we cannot
say that there is a probability greater than 0.95 of EBO-CMAR
outperforming jSO, nor that there is no such probability. These
numbers represent the area under the curve of the upper and lower
distributions when $P(X \leq Y) \geq 0.5$. In the last figure we can
see both posterior distributions. 

## Bayesian Multiple Measures Test

In the Bayesian version of the Multiple Measures test the results are
analogous to the frequentist version shown previously. We use the same
matrices with the results of each algorithm arranged by
dimensions. Here we obtain that the most probable configuration is
also the dominance of EBO in the 10, 30 and 100 dimensions while the
algorithm jSO obtains better results in the 50 dimension according to
this test, but the posterior probability is 0.84, as is shown in the
following table, so we cannot say that the difference with respect to
the remaining configurations is determinative.


```{r bayesian.multiple}
bmultiple.results <- bayesianMultipleConditions.test(matrix.EBO, matrix.jSO)
knitr::kable(bmultiple.results$probabilities)
```

# Summary of results in CEC'2017

pIn this section we include a summary of the statistical analysis
performed within the context of the CEC'2017.

The following table shows the official results of the algorithms in
their scoring system and the scores computed following the indications
of the report of the problem definition for the competition with the
available raw results of the algorithms. The difference between the
two scores may reside in the aggregation method for the results of the
different iterations. In this paper we have used the mean result,
while the organisers of the competition may have selected the best
result. There are some relevant differences, specially with respect to
the first score, that generate discrepancies in the classification of
the algorithms, for example LSHADE variants are benefited by the
aggregation of the raw results in detriment of EBO and jSO. In the
official CEC'17 summary, the results of the MOS12 algorithm are not
included, so we have excluded them in the analyses of this section.

\begin{table}[ht]
\centering
\begin{tabular}{|lrrr|lrrr|}
  \hline
  \multicolumn{4}{|c|}{Official CEC'17 Results} & \multicolumn{4}{c|}{Score Computation}\\
  \hline
  Algorithm & Score1 & Score2 & Score & Algorithm & Score1 & Score2 & Score \\ 
  \hline
  EBO & \textbf{50.00} & 48.01 & \textbf{98.01} & LSSPA & \textbf{50.00} & \textbf{50.00} & \textbf{100.00} \\ 
  jSO & 49.69 & 47.08 & 96.77 & LSCNE & 4.44 & 39.60 & 44.03 \\ 
  LSCNE & 46.82 & 49.74 & 96.56 & jSO & 4.71 & 38.68 & 43.39 \\ 
  LSSPA & 46.44 & \textbf{50.00} & 96.44 & EBO & 3.69 & 38.95 & 42.64 \\ 
  DES & 45.94 & 43.20 & 89.14 & DES & 4.35 & 36.93 & 41.28 \\ 
  MM & 45.96 & 40.12 & 86.07 & MM & 4.36 & 34.43 & 38.79 \\ 
  IDEN & 29.85 & 27.68 & 57.53 & RBI & 0.36 & 29.34 & 29.70 \\ 
  RBI & 3.79 & 33.61 & 37.40 & IDEN & 2.83 & 24.96 & 27.79 \\ 
  MOS13 & 18.94 & 17.34 & 36.28 & MOS11 & 1.05 & 17.72 & 18.77 \\ 
  MOS11 & 11.09 & 19.30 & 30.39 & MOS13 & 1.80 & 16.08 & 17.87 \\ 
  PPSO & 3.93 & 17.36 & 21.28 & PPSO & 0.37 & 16.07 & 16.44 \\ 
  DYYPO & 0.59 & 17.03 & 17.62 & DYYPO & 0.06 & 15.83 & 15.88 \\ 
  TFL & 0.03 & 16.25 & 16.27 & TFL & 0.00 & 15.01 & 15.01 \\ 
  \hline
\end{tabular}
\caption{CEC17 Results Scores with mean results} 
\label{tab:CEC17scores}
\end{table}

The classic statistical analysis that should be made in the context of
a competition would involve a non-parametric test with post-hoc test
for a $n$ versus $n$ scenario, as we do not have a preference for the
results of comparison of any specific algorithm. In order to preserve
the relative importance of the results in the different dimension
scenarios, we show the plots of the critical difference explained in
for the four scenarios.

As we can see in the following figures, summary scores are consistent
with Critical Differences plots, as the first classified algorithms
are also in the first positions of the graphical
representation. However, the statistical tests make it possible to
address the fact that there is a group of algorithms in the lead group
in every dimension scenario whose associated hypothesis of equivalence
cannot be discarded. These algorithms are: LSSPA, DES, LSCNE, MM and
jSO, which contradicts the fact that EBO won the competition. EBO has
the lowest mean rank in Dimension 10, 30 and 100, but the hypotheses
of equivalence with the first classified in Dimension 50 (LSSPA)
cannot be rejected.


```{r cd-plots, out.width = "40%"}
knitr::include_graphics("img/cd-plot-dim10.pdf")
knitr::include_graphics("img/cd-plot-dim30.pdf")
```

```{r cd-plots-2, out.width = "40%"}
knitr::include_graphics("img/cd-plot-dim50.pdf")
knitr::include_graphics("img/cd-plot-dim100.pdf")
```


In the Bayesian paradigm, after having rejected the equivalence of all
the mean ranks of the algorithms with the Friedman tests, we
repeatedly perform the Bayesian Signed-Rank for every pair of
algorithms in each dimension scenario. The results are summarized in
the following figures. Specially in lower dimensions there is less
certainty than there was in the non-parametric analysis concerning the
dominance of an algorithm over the other in each comparison, although
from the Bayesian perspective we can state where there is a tie
between a pair of algorithms and the direction of the difference,
while the equivalence with NHST cannot be assured. In this case, as we
are interested in the minimization of the result, an algorithm wins
when the difference between the results is in the left region. This is
represented by the color red, while the probability represents the
opacity of the tile.

## Dim 10
```{r}
bsr.results.dim10 <- cec17.final %>%
   filter(Dimension == 10, Algorithm != "MOS12") %>% select(-Dimension) %>%
   tidyr::spread("Algorithm", "Result") %>%
   select(-Benchmark) %>%
   apply.paired.bayesian(rNPBST::bayesianSignedRank.test, rope.min = -10, rope.max = 10)
```

```{r}
bsr.results.dim10.df <- bsr.results.dim10 %>%
   mutate(Winner = case_when(left > rope & left > right ~ "Alg. 1",
                             rope > left & rope > right ~ "rope",
                             right > left & right > rope ~ "Alg. 2"),
          Max.Prob = pmax(left, rope, right)) %>%
   select(Algorithm.1, Algorithm.2, Winner, Max.Prob)

bsr.results.dim10.df.rev <- bsr.results.dim10.df %>%
   transform(Algorithm.1 = Algorithm.2, Algorithm.2 = Algorithm.1,
             Winner = case_when(Winner == "Alg. 1" ~ "Alg. 2",
                                Winner == "Alg. 2" ~ "Alg. 1",
                                TRUE ~ "rope"))

bsr.results.dim10.df <- rbind.data.frame(bsr.results.dim10.df, bsr.results.dim10.df.rev) %>%
   mutate(Algorithm.1 = as.character(Algorithm.1),
          Algorithm.2 = as.character(Algorithm.2),
          Algorithm.1 = as.factor(Algorithm.1),
          Algorithm.2 = as.factor(Algorithm.2))

ggplot(bsr.results.dim10.df, 
       aes(x = Algorithm.2, y = Algorithm.1, fill = Winner)) +
   scale_x_discrete(limits = levels(bsr.results.dim10.df$Algorithm.1)) +
   scale_y_discrete(limits = rev(levels(bsr.results.dim10.df$Algorithm.1))) +
   geom_tile(aes(alpha = Max.Prob)) +
   geom_text(aes(label=format(Max.Prob, digits = 2)), alpha = 1) +
   labs(title = "Dimension 10",
        x = "Algorithm 2",
        y = "Algorithm 1") +
   scale_alpha(guide = "none")
```


## Dim 30

```{r}
bsr.results.dim30 <- cec17.final %>%
   filter(Dimension == 30, Algorithm != "MOS12") %>% select(-Dimension) %>%
   tidyr::spread("Algorithm", "Result") %>%
   select(-Benchmark) %>%
   apply.paired.bayesian(rNPBST::bayesianSignedRank.test, rope.min = -10, rope.max = 10)
```

```{r}
bsr.results.dim30.df <- bsr.results.dim30 %>%
   mutate(Winner = case_when(left > rope & left > right ~ "Alg. 1",
                             rope > left & rope > right ~ "rope",
                             right > left & right > rope ~ "Alg. 2"),
          Max.Prob = pmax(left, rope, right)) %>%
   select(Algorithm.1, Algorithm.2, Winner, Max.Prob)

bsr.results.dim30.df.rev <- bsr.results.dim30.df %>%
   transform(Algorithm.1 = Algorithm.2, Algorithm.2 = Algorithm.1,
             Winner = case_when(Winner == "Alg. 1" ~ "Alg. 2",
                                Winner == "Alg. 2" ~ "Alg. 1",
                                TRUE ~ "rope"))

bsr.results.dim30.df <- rbind.data.frame(bsr.results.dim30.df, bsr.results.dim30.df.rev) %>%
   mutate(Algorithm.1 = as.character(Algorithm.1),
          Algorithm.2 = as.character(Algorithm.2),
          Algorithm.1 = as.factor(Algorithm.1),
          Algorithm.2 = as.factor(Algorithm.2))

ggplot(bsr.results.dim30.df, 
       aes(x = Algorithm.2, y = Algorithm.1, fill = Winner)) +
   scale_x_discrete(limits = levels(bsr.results.dim10.df$Algorithm.1)) +
   scale_y_discrete(limits = rev(levels(bsr.results.dim10.df$Algorithm.1))) +
   geom_tile(aes(alpha = Max.Prob)) +
   geom_text(aes(label=format(Max.Prob, digits = 2)), alpha = 1) +
   labs(title = "Dimension 30",
        x = "Algorithm 2",
        y = "Algorithm 1") +
   scale_alpha(guide = "none")
```

## Dim 50


```{r}
bsr.results.dim50 <- cec17.final %>%
   filter(Dimension == 50, Algorithm != "MOS12") %>% select(-Dimension) %>%
   tidyr::spread("Algorithm", "Result") %>%
   select(-Benchmark) %>%
   apply.paired.bayesian(rNPBST::bayesianSignedRank.test, rope.min = -10, rope.max = 10)
```

```{r}
bsr.results.dim50.df <- bsr.results.dim50 %>%
   mutate(Winner = case_when(left > rope & left > right ~ "Alg. 1",
                             rope > left & rope > right ~ "rope",
                             right > left & right > rope ~ "Alg. 2"),
          Max.Prob = pmax(left, rope, right)) %>%
   select(Algorithm.1, Algorithm.2, Winner, Max.Prob)

bsr.results.dim50.df.rev <- bsr.results.dim50.df %>%
   transform(Algorithm.1 = Algorithm.2, Algorithm.2 = Algorithm.1,
             Winner = case_when(Winner == "Alg. 1" ~ "Alg. 2",
                                Winner == "Alg. 2" ~ "Alg. 1",
                                TRUE ~ "rope"))

bsr.results.dim50.df <- rbind.data.frame(bsr.results.dim50.df, bsr.results.dim50.df.rev) %>%
   mutate(Algorithm.1 = as.character(Algorithm.1),
          Algorithm.2 = as.character(Algorithm.2),
          Algorithm.1 = as.factor(Algorithm.1),
          Algorithm.2 = as.factor(Algorithm.2))

ggplot(bsr.results.dim50.df, 
       aes(x = Algorithm.2, y = Algorithm.1, fill = Winner)) +
   scale_x_discrete(limits = levels(bsr.results.dim50.df$Algorithm.1)) +
   scale_y_discrete(limits = rev(levels(bsr.results.dim50.df$Algorithm.1))) +
   geom_tile(aes(alpha = Max.Prob)) +
   geom_text(aes(label=format(Max.Prob, digits = 2)), alpha = 1) +
   labs(title = "Dimension 50",
        x = "Algorithm 2",
        y = "Algorithm 1") +
   scale_alpha(guide = "none")
```

## Dim 100


```{r}
bsr.results.dim100 <- cec17.final %>%
   filter(Dimension == 100, Algorithm != "MOS12") %>% select(-Dimension) %>%
   tidyr::spread("Algorithm", "Result") %>%
   select(-Benchmark) %>%
   apply.paired.bayesian(rNPBST::bayesianSignedRank.test, rope.min = -10, rope.max = 10)
```

```{r}
bsr.results.dim100.df <- bsr.results.dim100 %>%
   mutate(Winner = factor(case_when(left > rope & left > right ~ "Alg. 1",
                             rope > left & rope > right ~ "rope",
                             right > left & right > rope ~ "Alg. 2"),
                          levels = c("Alg. 1", "Alg. 2", "rope")),
          Max.Prob = pmax(left, rope, right)) %>%
   select(Algorithm.1, Algorithm.2, Winner, Max.Prob)

bsr.results.dim100.df.rev <- bsr.results.dim100.df %>%
   transform(Algorithm.1 = Algorithm.2, Algorithm.2 = Algorithm.1,
             Winner = case_when(Winner == "Alg. 1" ~ "Alg. 2",
                                Winner == "Alg. 2" ~ "Alg. 1",
                                TRUE ~ "rope"))

bsr.results.dim100.df <- rbind.data.frame(bsr.results.dim100.df, bsr.results.dim100.df.rev) %>%
   mutate(Algorithm.1 = as.character(Algorithm.1),
          Algorithm.2 = as.character(Algorithm.2),
          Algorithm.1 = as.factor(Algorithm.1),
          Algorithm.2 = as.factor(Algorithm.2))

ggplot(bsr.results.dim100.df, 
       aes(x = Algorithm.2, y = Algorithm.1, fill = Winner)) +
   scale_x_discrete(limits = levels(bsr.results.dim100.df$Algorithm.1)) +
   scale_y_discrete(limits = rev(levels(bsr.results.dim100.df$Algorithm.1))) +
   geom_tile(aes(alpha = Max.Prob)) +
   geom_text(aes(label=format(Max.Prob, digits = 2)), alpha = 1) +
   scale_fill_discrete(drop = FALSE) +
   labs(title = "Dimension 100",
        x = "Algorithm 2",
        y = "Algorithm 1") +
   scale_alpha(guide = "none")
```





```{r, out.width = "500px"}
knitr::include_graphics("img/bayesian-multiple-dim10-eps-converted-to.pdf")
```

As the error obtained in the competition increases, more comparisons
are marked as significant and less ties between algorithms are
detected. In this figure we see how in the comparison of the first
classified algorithms the most probable situation is a tie. This group
starts to win with a greater probability in the 30 Dimension scenario,
while the ties persist within the lead group.

```{r, out.width = "500px"}
knitr::include_graphics("img/bayesian-multiple-dim30-eps-converted-to.pdf")
```

The results of the 50 dimension scenario, shown in
the following figure, coincide with the conclusions
obtained in the non-parametric analysis, whilst the EBO algorithm, the
winner of the competition, obtains worse results (with a probability
above 0.8) than the lead group comprised of the DES, jSO, LSSPA, LSCNE
and MM algorithms. Finally in 100 dimension scenario, EBO wins in the
comparisons with all the remaining algorithms, with probabilities
between 0.64 in the comparison versus DES and 0.96 versus DYYPO.

```{r, out.width = "500px"}
knitr::include_graphics("img/bayesian-multiple-dim50-eps-converted-to.pdf")
```


```{r, out.width = "500px"}
knitr::include_graphics("img/bayesian-multiple-dim100-eps-converted-to.pdf")
```


# Appendix

```{r}
rNPBST::cec17.final %>% 
   filter(Dimension == 10) %>% 
   select(-Dimension) %>%
   tidyr::spread(Algorithm, Result) %>%
   xtable::xtable()
```


```{r}
aux.S1 <- rNPBST::cec17.final %>% 
   filter(Algorithm != "MOS12") %>%
   group_by(Algorithm, Dimension) %>%
   summarise(SE_dim = sum(Result)) %>%
   tidyr::spread(Dimension, SE_dim, sep=".") %>%
   mutate(SE = sum(0.1* Dimension.10, 0.2*Dimension.30, 0.3*Dimension.50, 0.4*Dimension.100)) %>%
   select(Algorithm, SE)
S1 <- aux.S1 %>%
   mutate(S1 = 50*min(aux.S1$SE)/SE) %>% select(-SE)

aux.S2 <- rNPBST::cec17.final %>% 
   filter(Algorithm != "MOS12") %>%
   group_by(Benchmark, Dimension) %>%
   mutate(Rank = rank(Result)) %>% 
   ungroup() %>%
   select(Algorithm, Dimension, Rank) %>%
   group_by(Algorithm, Dimension) %>%
   summarise(SE_dim = sum(Rank)) %>%
   tidyr::spread(Dimension, SE_dim, sep=".") %>%
   mutate(SE = sum(0.1* Dimension.10, 0.2*Dimension.30, 0.3*Dimension.50, 0.4*Dimension.100)) %>%
   select(Algorithm, SE)
S2 <- aux.S2 %>%
   mutate(S2 = 50*min(aux.S2$SE)/SE) %>% select(-SE)
```

```{r}
FinalScore <- inner_join(S1, S2) %>% mutate(Final = S1+S2) %>% arrange(desc(Final))
FinalScore
```


