---
title: "Statistical Analysis"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Statistical Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, echo = FALSE, error= TRUE)
```


```{r load.libraries, message=FALSE, warning=FALSE}
library(rNPBST)
library(dplyr)
library(scmamp)
library(xtable)
library(ggplot2)
```

# Parametric analysis

In this section we illustrate the application of the parametric tests
to the comparison of algorithms in the study case of the competition
of CEC2017. With this purpose, we use the results obtained at the end
of the 100\% of the steps and compute the mean among different
iterations of each algorithm on each benchmark function.

## Preliminars

The traditional parametric test used in the context of a comparison of
multiple algorithms over multiple problems (benchmarks) is the ANOVA
test. This test makes some assumptions that should be checked before
it is performed:
- The distribution of the results among different benchmarks is normal.
- The standard deviation of results is equal. 

### Normality


```{r shapiro, echo=TRUE, results="asis"}
shapiro.test <- split(cec17.final, cec17.final$Dimension) %>%
   lapply(function(x) split(x, x$Algorithm) %>%
   lapply(function(y) shapiro.test(y$Result)$p.value)) %>%
   unlist() %>% matrix(ncol = 4, byrow = F) %>% as.data.frame()

AdjustFormatTable(shapiro.test, 
                  colnames = paste("Dim.", levels(cec17.final$Dimension)),
                  rownames = levels(cec17.final$Algorithm), print.code = TRUE,
                  digits = -2, type = "latex")
```    

We can see that all the null hypotheses are rejected because the
associated p-values are less than $0.05$. This means that the mean
results of the algorithms among the different iterations do not follow
a normal distribution, as could be expected due to the different
difficulty of the multiple benchmark functions throug the dimensions.

### Homocedasticity

```{r levene}
levene.test <- split(cec17.final, cec17.final$Dimension) %>%
  lapply(function(x) lawstat::levene.test(x$Result,  x$Algorithm)$p.value) %>% 
  unlist() %>% matrix(ncol = 4, byrow = T)
AdjustFormatTable(levene.test, type ="latex", digits = -2, 
                  colnames = paste("Dim.", levels(cec17.final$Dimension)),
                  rownames = "p.value", caption = "Levene test for homocedasticity",
                  label = "tab:levene", print.code = TRUE)
```

Although normality is not satisfied, we check the homocedasticity for
the different dimensions. As the $p$-value are greater than $\alpha =
0.05$ we cannot reject the hypothesis of the equivalence of the
variances.


```{r t.test}
split(cec17.final, cec17.final$Dimension) %>%
   lapply(function(x){
      y <- t.test(select(filter(x, Algorithm == "EBO"), Result),
                  select(filter(x, Algorithm == "jSO"), Result))$p.value
   }) %>%
    unlist() %>%
    matrix(nrow = 1, dimnames = list("p.value", paste("Dim.", levels(cec17.final$Dimension)))) %>%
    AdjustFormatTable(caption = "EBO vs. jSO t-test results", label = "tab:t-test")
```


```{r anova}

split(cec17.final, cec17.final$Dimension) %>%
   lapply(function(x){
      y <- aov(Result ~ Algorithm, x) %>% summary()
      y[[1]][1,5]
   }) %>%
    unlist() %>%
    matrix(nrow = 1, dimnames = list("p.value", paste("Dim.", levels(cec17.final$Dimension)))) %>%
    AdjustFormatTable(caption = "ANOVA results", label = "tab:ANOVA")
```

We perform the ANOVA test and see how the equivalence of the means by
algorithms can not be rejected for any dimensions, a circumstance that
is consistent with the previous statements about the power of the
ANOVA test when the assumption of the test are not fulfilled.

# Non-parametric tests

In this section we perform the most popular tests in the field of the
comparison of optimization algorithms. We continue using the
aggregated results across the different runs at the end of the
iterations, except for the Page test for the study of the convergence.

## Classic tests

```{r}
EBO <- unlist(select(filter(cec17.final, Algorithm == "EBO", Dimension == 10), Result), use.names = F)
jSO <- unlist(select(filter(cec17.final, Algorithm == "jSO", Dimension == 10), Result), use.names = F)
```


```{r}
sign.results <- rNPBST::binomialSign.test(cbind(EBO, jSO))
wilcoxon.results <- rNPBST::wilcoxon.test(cbind(EBO, jSO))
wilcoxonRS.results <- rNPBST::wilcoxonRankSum.test(cbind(EBO, jSO))

r
p.values <- sapply(list(sign.results,wilcoxon.results, wilcoxonRS.results), function(x) x$p.value[3])
lapply(list(sign.results,wilcoxon.results, wilcoxonRS.results), function(x) x$statistic)


```





```{r}

ApplyNPPairwise <- function(data, test){
split(data, data$Dimension) %>%
   lapply(function(x){
      y <- test(matrix(c(unlist(select(filter(x, Algorithm == "EBO"), Result)),
                                              unlist(select(filter(x, Algorithm == "jSO"), Result))),
                                            ncol = 2))$p.value[3]
   }) %>%
    unlist
}

results.sign <- ApplyNPPairwise(cec17.final, rNPBST::binomialSign.test)
results.wilcoxon <- ApplyNPPairwise(cec17.final, rNPBST::wilcoxon.test)
results.wilcoxonRS <- ApplyNPPairwise(cec17.final, rNPBST::wilcoxonRankSum.test)

results.nppairwise <- matrix(c(results.sign, results.wilcoxon,results.wilcoxonRS),
                             nrow = 3, byrow = T,
                             dimnames = list(c("Sign","Wilcoxon", "Wilcoxon Rank-Sum"), 
                                             paste("Dim.", levels(cec17.final$Dimension)))) %>%
    AdjustFormatTable(caption = "Non-parametric pairwise comparison between EBO and jSO", label = "tab:np-pairwise", print.code = T)
```



```{r apply.np.test}
ApplyNPTest <- function(data, test){
    split(data, data$Dimension) %>%
    lapply(function(x)
        select(x, -Dimension) %>% 
        tidyr::spread(Algorithm, Result) %>%
        select(-Benchmark) %>%
        test %>% get("p.value", .)) %>%
    unlist
}
```
    

```{r np.test}
friedman.results <- ApplyNPTest(cec17.final, friedman.test)
friedman.ar.results <- ApplyNPTest(cec17.final, friedmanAR.test)
iman.davenport.results <- ApplyNPTest(cec17.final, imanDavenport.test)
quade.results <- split(cec17.final, cec17.final$Dimension) %>%
  lapply(function(x){
    x <- select(x, -Dimension)
    quade.test(x$Result, x$Algorithm, x$Benchmark) %>% get("p.value", .)
  }) %>%
  unlist(use.names = T)
```    


```{r np.results}
non.parametric <- matrix(c(friedman.results, friedman.ar.results,
                           iman.davenport.results, quade.results),
                         byrow = T, ncol = 4)

AdjustFormatTable(non.parametric,
                  colnames = paste("Dim.", levels(cec17.final$Dimension)),
                  rownames = c("Friedman" ,"Friedman AR", "Iman-Davenport", "Quade"),
                  caption = "Non-Parametric tests",
                  label = "tab:np-tests")
```
We can see how the four tests reject the null hypotheses, it is, the
equivalence of the medians of the results of the different
benchmarks. We must have in mind that a comparison between thirteen
algorithms is not the recommended procedure if we want to compare our
proposal, but we should include in the comparison the state-of-the-art
algorithms, because the inclusion of an algorithm with lower
performance will be reflected in the rejection of the null hypothesis
while that would be not rejected if this bogus algorithm would not
have been included. 

## Post-hoc tests

Then, we proceed to perform the post-hoc tests in order to determinate
the location of the differences between these algorithms. We use the
modification of the classic non-parametric tests to obtain the
$p$-value associated with each hypothesis, although we should adjust
these $p$-values with a post-hoc procedure.


### Control algorithm 

To illustrate this, we first suppose that we are in an One versus all
scenario where we are presenting our algorithm (we are going to use
EBOwithCMAR, the winner of the CEC'2017 competition). The possible
approach here, as in the rest of the analysis are:

- Considering all the results in the different dimensions as if they
  were different function or benchmarks, we would only obtain a single
  $p$-value for the comparison between EBO-CMAR with each contestant
  algorithm. The adjusted $p$-values are shown in the following table
  for the Friedman, Friedman Aligned-Rank and Quade test.


```{r post.hoc}
cec17.final.10 <- filter(cec17.final, Dimension == 10)
ApplyNPTestPostHoc <- function(data, test, adjust, control = NULL){
    data %>%
        tidyr::spread(Algorithm, Result) %>%
        select(-Benchmark, -Dimension) %>%
        test(control = control) %>%
        adjust
}

results.post.friedman <- ApplyNPTestPostHoc(cec17.final, scmamp::friedmanPost,
                                           scmamp::adjustHolland, control = "EBO")
results.post.friedmanAR <- ApplyNPTestPostHoc(cec17.final, scmamp::friedmanPost,
                                             scmamp::adjustHolland, control = "EBO")
results.post.quade <- ApplyNPTestPostHoc(cec17.final, scmamp::quadePost, scmamp::adjustHolland,
                                        control = "EBO")


results.post.global.holland <- rbind(results.post.friedman,
                                    results.post.friedmanAR,
                                    results.post.quade)

AdjustFormatTable(as.data.frame(t(results.post.global.holland)),
                  colnames = c("Friedman","FriedmanAR","Quade"),
                  caption = "Post-Hoc Non-Parametric tests with control algorithm",
                  label = "tab:np-tests-ph-control", print.code = T)
```

- If we wanted to show that the differences between the algorithms
  also persist in each group of results obtained across the different
  dimensions, we should perform these tests repeatedly and apply later
  the appropiate post-hoc procedure. In the following table we show
  the adjusted $p$-value obtained.

```{r post.hoc.grouped}
ApplyNPTestPostHoc.Grouped <- function(data, test, adjust, control = NULL){
  results <- split(data, data$Dimension) %>%
    sapply(function(x)
      select(x, -Dimension) %>% 
        tidyr::spread(Algorithm, Result) %>%
        select(-Benchmark) %>%
        test(control = control)) %>%
    adjust
  return(results)
}
post.hoc.grouped <- ApplyNPTestPostHoc.Grouped(cec17.final, 
                                               scmamp::friedmanPost, 
                                               scmamp::adjustHolland,
                                              control = "EBO")

AdjustFormatTable(post.hoc.grouped,
                  colnames = paste("Dim", colnames(post.hoc.grouped)),
                  rownames = levels(cec17.final$Algorithm),
                  caption = "Results grouped by dimension. Friedman test + Holland adjust.",
                  label = "tab:grp-friedman-holland")
```

### $n$ versus $n$ scenario

In the scenario of the statistical analysis of the results obtained
during a competition, we would not focus in the comparison between the
results of a single algorithm but we would make all the posible pairs,
so we would not use the control algorithm.

As in the scenario with a control algorithm, we could be interested in
the  global comparison  between  all the  posible  pairs of  algorithm
considering the  results in the  different dimensions as if  they came
from the same distribution or pay attention to the differences between
the algorithms in the different contexts according to the dimension.

```{r all.vs.all}
ApplyNPTestPostHoc(cec17.final.10,
                   scmamp::friedmanPost,
                   scmamp::adjustHolland)  %>%
  AdjustFormatTable(caption = "Results $n$ vs $n$. Friedman test + Holland adjust.",
                    label = "tab:nvsn-friedman-holland", print.code = F)
```

```{r all.vs.all.group}
all.vs.all.group <- scmamp::postHocTest(select(tidyr::spread(cec17.final, Algorithm, Result),
                                             -Benchmark),
                                      group.by = "Dimension",
                                      test = "friedman", adjust = "holland")

all.vs.all.group$corrected.pval[, ,1] %>%
  AdjustFormatTable(caption = "Results $n$ vs $n$, dimension 10. Friedman test + Holland adjust.",
                    label = "tab:nvsn-dim10-friedman-holland")

# all.vs.all.group$corrected.pval[, ,3] %>%
#   AdjustFormatTable(caption = "Results $n$ vs $n$, dimension 30. Friedman test + Holland adjust.",
#                     label = "tab:nvsn-dim30-friedman-holland")
```

## Convergence test

For the comparison of the convergence of two algorithms, we use the
mean result acrossthe different runs for each benchmark function of
two algorithms. This results could be equally extended using the
convenient adjustments. It is relevant to note that the algorithms
EBOwithCMAR (first classified) and LSHADE-cnEpSin (third classified)
do not provide their partial results, so we compare here the
convergence of the jSO and LSHADE\_SPACMA algorithms in the
competition for the 10 dimension scenario, where the null hypothesis
of LSHADE\_SPACMA cannot be rejected and 50 dimension scenario.

```{r convergence}
convergence.comparison.lshade <- filter(cec17.mean,
                                Algorithm == "LSSPA",
                                Dimension == 10) %>%
    select(-Dimension)
convergence.comparison.jso <- filter(cec17.mean,
                                Algorithm == "jSO",
                                Dimension == 10) %>%
    select(-Dimension)

convergence.comparison <- rbind(convergence.comparison.jso,
                                convergence.comparison.lshade) %>%
  as.data.frame() %>% reshape2::melt(id.vars = c("Algorithm", "Benchmark"), value.name = "Result") %>%
  transform(Perc.Iterations = as.numeric(gsub("perc.completion.", "", variable))) %>% select(-variable)

convergence.comparison %>%
  filter(Benchmark %in% c(3, 9, 11, 15)) %>%
  ggplot(aes(x = Perc.Iterations, y = Result, col = Benchmark, linetype = Algorithm)) + 
  geom_line()
```

```{r}
plot(as.numeric((convergence.comparison.lshade - convergence.comparison.jso)[2, ]))
```


```{r}
convergence.comparison.jso <- select(convergence.comparison.jso, -Benchmark, -Algorithm)
convergence.comparison.lshade <- select(convergence.comparison.lshade, -Benchmark, -Algorithm)
cat(rNPBST::htest2Tex(rNPBST::page.test(-convergence.comparison.lshade+convergence.comparison.jso)))
```




```{r}
convergence.comparison.jso <- filter(cec17.mean,
                                Algorithm == "jSO",
                                Dimension == 100) %>%
    select(-Algorithm, -Benchmark, -Dimension)
convergence.comparison.dyypo <- filter(cec17.mean,
                                Algorithm == "DYYPO",
                                Dimension == 100) %>%
    select(-Algorithm, -Benchmark, -Dimension)
rNPBST::page.test(convergence.comparison.jso - convergence.comparison.dyypo)
rNPBST::page.test(-convergence.comparison.jso + convergence.comparison.dyypo)
```


## Confidence Intervals and Confidence Curves

In this Section we show the use of confidence intervals and confidence
curves in the comparison of optimization results. First, we have to
warn that these comparisons only take care of two algorithm at each
time, and a post-hoc correction would be needed if the comparison
involved a greater number of algorithms as we are going to see in
the following examples. 

- We suppose that we want to compare the final and mean results of the
  two first classified algorithms in the benchmark function for the 10
  dimension. Then, we select in the dataset the corresponding data and
  obtain the following result.

```{r confidence.intervals}
conf.interval.PPSO <- filter(cec17.final,Algorithm == "PPSO",
                           Dimension == 10) %>%
    select(Result)
conf.interval.jSO <- filter(cec17.final,Algorithm == "jSO",
                           Dimension == 10) %>%
    select(Result)
```

As the 0 effect is not included in the Non-parametric confidence
interval, the null hypothesis is then rejected, although the interval
is very wide, so we have not much certainty about that. If we only had
done the Wilcoxon test, we would have obtained the associated
$p$-value, and the null hypothesis would be also rejected, so the
difference between the medians are detected with both methods. If we
look at the confidence curve, we can reject the classic null
hypothesis if the intersection of the vertical line at the 0 level and
the horizontal line at the $\alpha$ value remains under the curve. 

```{r confidence.curve}
conf.interval.matrix <- cbind(conf.interval.PPSO, conf.interval.jSO)
colnames(conf.interval.matrix) <- c("PPSO", "jSO")
cat(rNPBST::htest2Tex(rNPBST::wilcoxon.test(conf.interval.matrix)))
```    

```{r}
ApproximateNPConfidenceInterval(x = conf.interval.matrix[ ,1],
                                                           y = conf.interval.matrix[ ,2], paired = T)
PlotConfidenceCurve(x = conf.interval.matrix[ ,1],
                                               y = conf.interval.matrix[ ,2], m = 50, paired = T)
```

## Multiple measures test - GLRT

```{r multiple.measures.nonparametric}
matrix.EBO <-  filter(cec17.final, Algorithm == "EBO") %>%
    select(-Algorithm) %>% tidyr::spread(Dimension, Result) %>%
    select(-Benchmark)
matrix.jSO <-  filter(cec17.final, Algorithm == "jSO") %>%
    select(-Algorithm) %>% tidyr::spread(Dimension, Result) %>%
    select(-Benchmark)
rNPBST::multipleMeasuresGLRT(matrix.EBO, matrix.jSO) 
```

In order to perform the Non-Parametric Multiple Measures test, we
consider that the results of the different dimensions as if they were
different measures in the same benchmark function. We select the means
of the executions of the two best algorithms and reshape them into a
matrix with the results of each benchmark in the rows and the
different dimensions in the columns. Then we use the test to see which
hypothesis of dominance is the most probable and if we can state that
the probability of this dominance statement is significant. According
to the results, we obtain that the most observed dominance statement
in the third position (which corresponds to the number 2, 0010 in
binary) and the configuration $[<,<,>,<]$, it is, EBO-CMAR obtains a
better result for the dimensions 10, 30 and 100 while jSO obtains
better results in the 50 dimension scenario. However, the associated
null hypothesis which states that the mentioned configuration is no
more probable than the following one, obtain a $p$-value of 0.25, so
this hypothesis cannot be rejected. The second most probable
configuration is in the 15th position (or number 14) and the
configuration $[>,>,>,<]$, which means that EBO-CMAR obtains worse
results in dimensions 10,30 and 50, so we cannot assure which is the
most probable situation in the lower dimension scenarios.

# Bayesian tests

In this section we illustrate the use of the proposed Bayesian
tests. The considerations are analogous to the ones made in the
frequentist case, as the described tests use the aggregations of the
runs to compare the results of the different benchmark functions, or
use these runs with the drawback of obtaining a restrained statement
about the results in one single problem.

## Bayesian Friedman test

We start with the Bayesian version of the Friedman test. In this test
we do not obtain a single $p$-value, but the accepted hypothesis. Due
to the high number of contestant algorithms and the memory needed to
allocate the covariance matrix of all the possible permutations, we
perform here the imprecise version of the test. The null hypothesis of
the equivalence of the mean ranks for the 10 dimension scenario is
rejected.

```{r bayesian.friedman.test}
dim.10 <- filter(cec17.final, Dimension == 10) %>%
    tidyr::spread(Algorithm, Result) %>% select(-Benchmark, -Dimension)

bft.results <- bayesianFriedman.test(dim.10, imprecise = TRUE, n.samples = 200)
bft.results$h
meanranks <- bft.results$meanranks
names(meanranks) <- colnames(bft.results$covariance)
xtable::xtable(as.matrix(meanranks))
```

## Bayesian Sign and Signed-Rank test

The originial proposal of the use of the Bayesian Sign and Signed-Rank
tests are destinated to the comparison of classification algorithms
and the proposed region of practical equivalence is $[-0.01,0.01]$ for
a measure in the range $[0,1]$. In the scenario of optimization
problems, we should be concern that the possible outcomes are
lower-bounded by 0 but in many functions there is not an upper bound
or the maximum is very high so we must follow another approach. As the
difference in the 30 dimension comparison is between 0 and 1527, we
state that the region of practical equivalence is $[-10,10]$.


```{r bayesian.pair.test}
jso <- filter(cec17.final, Algorithm == "jSO", Dimension == 10) %>%
    select(Result) %>% unlist()
ebo <- filter(cec17.final, Algorithm == "EBO", Dimension == 10) %>%
    select(Result) %>% unlist()

bst.results <- rNPBST::bayesianSign.test(ebo, jso,
                                       rope.min = -10, rope.max = 10)
plot(bst.results, num.points = 10000) + ggplot2::labs(x = "jSO", z = "EBO")

bsrt.results <- rNPBST::bayesianSignedRank.test(ebo, jso,
                                              rope.min = -10, rope.max = 10)
plot(bsrt.results, num.points = 10000)+ ggplot2::labs(x = "jSO", z = "EBO", y = "rope")
```

The test compute the probability of the true location of
$\textrm{EBO-CMAR} - \textrm{jSO}$ with respect to $0$, so both tests'
results gives that the left region (jSO obtain worse results) a
greater probability, although this is not significative. Rope
probability is low in comparison with the right and left region, and
this is because, although the number of wins and losses is similar for
both algorithms, the number of benchmark functions where the results
are quite similar is lower.

We can see the posterior probability density of the parameter in the
following plots, where each point represents an estimation of the
probability of the parameter to belong to each region of
interest. This means that we have repeatedly obtained the triplets of
the probability of each region to be the true location of the
difference between the two samples and then we have plotted these
triplets to obtain the posterior distribution of the parameter. If we
compare these results with a paired Wilcoxon test we see that the null
hypothesis of the equivalence of the means cannot be
rejected. However, using the Bayesian paradigm we cannot establish the
dominance of one algorithm over the other either, but while in the
frequentist paradigm we could be tempted to (erroneously) establish
that there is no difference between these algorithms with these plots
we obtain valuable information and we can see that here is not the same
situation, as the probability of the rope is very low.


```{r plot.pair.bayesian.tests}
wilcox.test(jso, ebo.cmar, paired = T)
plot(bst.results, num.points = 10000) 
plot(bsrt.results, num.points = 10000)
```

## Imprecise Dirichlet Process Test

The Imprecise Dirichlet Process consists in a more complex idea of the
previous tests although the implications of the use of the Bayesian
Tests could be clearer. In this test we try to not to introduce any
prior distribution, not even the prior distribution where both
algorithms have the same performance, but all the possible probability
measures $\alpha$, and then obtain an upper and a lower bounds for the
probability in which we are interested. The input consits in the
aggregated results among the different runs for all the benchmark
functions for a single dimension. The other parameters of the function
are the strong parameter $s$ and the pseudo-observation $c$. With
these data, we obtain two bounds for the posterior probability of the
first algorithm outperforming the second one, i.e. the probability of
$P(X \leq Y) \geq 0.5$. These are the possible scenarios:

- Both bounds are greater than 0.95: Then we can say that the first
  algorithm outperforms the second algorithm with a 95\% of
  probability.
- Both bounds are lower than 0.05: This is the inverse case. In this
  situation, the second algorithm outperforms the first algorithm with
  a 95\% of probability.
- Both bounds are between 0.05 and 0.95: Then we can say that the
  probability of one algorithm outperforming the other is lower than
  the desired probability of 0.95.
- Finally, if only one of the bounds is greater than 0.95 or lower
  than 0.05, the situation is indetermined and we cannot decide.

```{r bayesian.imprecise}
library(dplyr)
library(rNPBST)
jso <- filter(cec17.final, Algorithm == "jSO", Dimension == 50) %>%
    select(Result) %>% unlist()
ebo.cmar <- filter(cec17.final, Algorithm == "EBO", Dimension == 50) %>%
    select(Result) %>% unlist()

bayesian.imprecise.results <- bayesian.imprecise(jso, ebo.cmar)
plot(bayesian.imprecise.results) + ggplot2::ggtitle("JSO vs. EBO-CMAR")
```

According to the results of the Bayesian Imprecise Dirichlet Process,
the probability $\mathcal{P}[P(EBO-CMAR \leq jSO) \geq 0.5]$, that is
the probability of EBO-CMAR outperforming jSO, is between 0.92 and
0.96, so there is an indetermination and we cannot say that there is 
a probability greater than 0.95 of EBO-CMAR outperforming jSO, 
nor there is not such probability. These numbers represent
the area under the curve of the upper and lower distributions when
$P(X \leq Y) \geq 0.5$



## Bayesian Multiple Measures Test

In the Bayesian version of the Multiple Measures test the results are
analogous to the frequentist version. The most probable configuration
is also the dominance of the EBO-CMAR in the 10,30 and 100 dimensions
while the algorithm jSO obtains better results in the 50 dimension
according to this test, but the posterior probability is 0.841, so we
cannot say that the difference with respect the remaining
configurations is determinative.


```{r bayesian.multiple}
bayesianmultipleconditions.results <- rNPBST::bayesianMultipleConditions.test(a,b)
bayesianmultipleconditions.results$probabilities
```

## PlackettLuce

```{r}
library(rNPBST)
cec17.extended.10 <- filter(cec17.extended, Dimension == "10") %>% select(-Dimension)
a <- merge(cec17.extended.10, cec17.extended.10, by = c("Benchmark","Iteration")) %>%
  filter(Algorithm.x != Algorithm.y) %>%
  filter_at(vars(ends_with(".x"),
                 ends_with(".y")), 
            any_vars(. == 0)) %>%
  group_by(Algorithm.x, Algorithm.y) %>%
  mutate(End.x = sum(!!any_vars(. == 0)))
  summarise(r_ij = n(),
            w_ij = sum(vars(ends_with(".x")), any_vars(. == 0)))) %>%
  ungroup()
```

# Appendix

```{r}
base.data <- cec17.final %>%
  filter(Dimension == 10) %>%
  select(-Dimension) %>%
  reshape(idvar = "Benchmark", timevar = "Algorithm", direction = "wide") %>%
  transform(Benchmark = as.integer(levels(Benchmark))[Benchmark]) %>% arrange(Benchmark)
colnames(base.data)[-1] <- levels(cec17.final$Algorithm)
knitr::kable(base.data)
```
